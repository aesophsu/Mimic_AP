import os
import json
import joblib
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, confusion_matrix, f1_score, roc_auc_score

# =========================================================
# 1. åŸºç¡€é…ç½®
# =========================================================
BASE_DIR = "../../"
MODEL_ROOT = os.path.join(BASE_DIR, "artifacts/models")
FIG_ROOT = os.path.join(BASE_DIR, "results/figures")
TABLE_ROOT = os.path.join(BASE_DIR, "results/tables")
OUTCOMES = ['pof', 'mortality_28d', 'composite_outcome']

for path in [FIG_ROOT, TABLE_ROOT]:
    if not os.path.exists(path):
        os.makedirs(path)

def calculate_detailed_metrics(y_true, y_prob, threshold):
    """åŸºäºç‰¹å®šé˜ˆå€¼è®¡ç®—ä¸´åºŠè¯Šæ–­æŒ‡æ ‡"""
    y_pred = (y_prob >= threshold).astype(int)
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
    
    # è®¡ç®—ç‚¹ä¼°è®¡
    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
    
    metrics = {
        "Threshold": round(threshold, 4),
        "TP": int(tp), "FP": int(fp), "TN": int(tn), "FN": int(fn),
        "Sensitivity": round(sensitivity, 4),
        "Specificity": round(specificity, 4),
        "PPV": round(tp / (tp + fp), 4) if (tp + fp) > 0 else 0,
        "NPV": round(tn / (tn + fn), 4) if (tn + fn) > 0 else 0,
        "F1_Score": round(f1_score(y_true, y_pred), 4),
        "Accuracy": round((tp + tn) / (tp + tn + fp + fn), 4)
        # "Sen_CI": "N/A"  # å¦‚æœä¸è·‘ Bootstrapï¼Œå»ºè®®å…ˆæ³¨é‡Šæ‰æˆ–è®¾ä¸º N/A
    }
    return metrics

def plot_diagnostic_viz(y_true, y_prob, threshold, name, target, save_dir):
    """ç”Ÿæˆç§‘ç ”çº§ ROC æ ‡æ³¨å›¾å’Œæ¦‚ç‡åˆ†å¸ƒå›¾"""
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
        
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6), dpi=300)

    # 1. å¸¦æˆªæ–­ç‚¹çš„ ROC
    fpr, tpr, _ = roc_curve(y_true, y_prob)
    auc_val = roc_auc_score(y_true, y_prob)
    ax1.plot(fpr, tpr, label=f'ROC (AUC={auc_val:.3f})', color='darkorange', lw=2)
    ax1.plot([0, 1], [0, 1], 'k--', alpha=0.3)
    
    # è®¡ç®—å½“å‰é˜ˆå€¼ä¸‹çš„ Sen/Spe ç”¨äºæ ‡æ³¨
    perf = calculate_detailed_metrics(y_true, y_prob, threshold)
    ax1.scatter(1-perf['Specificity'], perf['Sensitivity'], color='red', s=100, 
                label=f'Best Cutoff: {threshold:.3f}\n(Sen:{perf["Sensitivity"]:.2f}, Spe:{perf["Specificity"]:.2f})')
    # åœ¨ ax1.scatter ä¹‹åæ·»åŠ ï¼Œå¢å¼ºç§‘ç ”æ„Ÿ
    ax1.annotate(f'Opt: {threshold:.3f}\nSen: {perf["Sensitivity"]:.2f}\nSpe: {perf["Specificity"]:.2f}',
                 xy=(1-perf['Specificity'], perf['Sensitivity']), 
                 xytext=(1-perf['Specificity']+0.1, perf['Sensitivity']-0.2),
                 arrowprops=dict(facecolor='black', shrink=0.05, width=1, headwidth=5))
    ax1.set_xlabel('1 - Specificity')
    ax1.set_ylabel('Sensitivity')
    ax1.set_title(f'{name} ROC Analysis ({target})')
    ax1.legend(loc='lower right')

    # 2. æ¦‚ç‡åˆ†å¸ƒå›¾ (å±•ç¤ºé£é™©åˆ†ç¦»åº¦)
    df_prob = pd.DataFrame({'prob': y_prob, 'target': y_true})
    ax2.hist(df_prob[df_prob['target'] == 0]['prob'], bins=40, alpha=0.5, label='Normal', color='blue', density=True)
    ax2.hist(df_prob[df_prob['target'] == 1]['prob'], bins=40, alpha=0.5, label='Outcome(+)', color='red', density=True)
    ax2.axvline(threshold, color='black', linestyle='--', lw=2, label=f'Cutoff: {threshold:.3f}')
    
    ax2.set_xlabel('Predicted Probability')
    ax2.set_ylabel('Density')
    ax2.set_title('Risk Separation Distribution')
    ax2.legend()

    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, f"07_Diagnostic_{name}.png"))
    plt.close()

# =========================================================
# 2. æ ¸å¿ƒæ‰§è¡Œé€»è¾‘
# =========================================================
def run_cutoff_optimization_flow():
    print("ğŸš€ å¯åŠ¨ 07 æ­¥ï¼šè‡ªåŠ¨åŒ–é˜ˆå€¼å¯»ä¼˜ä¸ä¸´åºŠæ•ˆèƒ½å®¡è®¡...")
    global_summary = []

    for target in OUTCOMES:
        target_dir = os.path.join(MODEL_ROOT, target)
        fig_save_dir = os.path.join(FIG_ROOT, target)
        
        if not os.path.exists(target_dir):
            print(f"âš ï¸ è·³è¿‡ {target}: æ‰¾ä¸åˆ°èµ„äº§ç›®å½•")
            continue

        print(f"\n--- æ­£åœ¨å¤„ç†ç»ˆç‚¹: [{target.upper()}] ---")
        
        try:
            # 1. åŠ è½½æ ¸å¿ƒèµ„äº§
            models_dict = joblib.load(os.path.join(target_dir, "all_models_dict.pkl"))
            eval_data = joblib.load(os.path.join(target_dir, "eval_data.pkl"))
            X_test_pre = eval_data['X_test_pre']
            y_test = eval_data['y_test']
            
            # 2. å¢å¼ºåŠŸèƒ½ 1ï¼šç‰¹å¾å¯¹é½é€»è¾‘ (ä¿®å¤ç¼©è¿›ä¸é€»è¾‘)
            feat_path = os.path.join(target_dir, "selected_features.json")
            if os.path.exists(feat_path):
                with open(feat_path, 'r') as f:
                    selected_features = json.load(f)
                
                # ç¡®ä¿ X_test çš„åˆ—é¡ºåºä¸è®­ç»ƒæ—¶ä¸€è‡´
                if isinstance(X_test_pre, pd.DataFrame):
                    X_test_pre = X_test_pre[selected_features]
                    print(f"  âœ… ç‰¹å¾å¼ºåˆ¶å¯¹é½æˆåŠŸ (n={len(selected_features)})")
                else:
                    print(f"  âš ï¸ è­¦å‘Š: {target} çš„æµ‹è¯•é›†ä¸æ˜¯ DataFrame æ ¼å¼ï¼Œæ— æ³•è‡ªåŠ¨æ’åºç‰¹å¾ã€‚")
            
        except Exception as e:
            print(f"âŒ åŠ è½½ {target} èµ„äº§å¤±è´¥: {e}")
            continue

        target_thresholds = {}
        target_perf_report = []

        # 3. æ¨¡å‹éå†å¾ªç¯
        for name, clf in models_dict.items():
            X_eval = X_test_pre.values if hasattr(X_test_pre, 'values') else X_test_pre
            y_prob = clf.predict_proba(X_eval)[:, 1]
            fpr, tpr, thresholds = roc_curve(y_test, y_prob)
            
            # åŠŸèƒ½ 2ï¼šYouden Index å¯»ä¼˜ä¸å¼‚å¸¸å¤„ç†
            if len(thresholds) <= 1:
                print(f"  âš ï¸ æ¨¡å‹ {name} é¢„æµ‹æ— åŒºåˆ†åº¦ï¼Œè®¾ç½®é»˜è®¤é˜ˆå€¼ 0.5")
                best_th = 0.5
            else:
                youden_index = tpr + (1 - fpr) - 1
                best_idx = np.argmax(youden_index)
                best_th = float(thresholds[best_idx])
            
            # ä¿®æ­£ sklearn æœ‰æ—¶ä¼šç”Ÿæˆé˜ˆå€¼ > 1.0 çš„æƒ…å†µ
            if best_th > 1.0: best_th = 1.0

            # åŠŸèƒ½ 3ï¼šå…¨ç»´åº¦æ•ˆèƒ½å®¡è®¡ (åŒ…å«æ··æ·†çŸ©é˜µè®¡æ•°)
            perf = calculate_detailed_metrics(y_test, y_prob, best_th)
            perf['Algorithm'] = name
            target_perf_report.append(perf)
            target_thresholds[name] = best_th

            # åŠŸèƒ½ 5ï¼šå¯è§†åŒ–è¯Šæ–­å›¾ (ROC + æ¦‚ç‡åˆ†å¸ƒ)
            plot_diagnostic_viz(y_test, y_prob, best_th, name, target, fig_save_dir)

            # è®°å½•åˆ°å…¨å±€æ±‡æ€»æ¸…å•
            global_summary.append({
                "Outcome": target,
                "Algorithm": name,
                "AUC": round(roc_auc_score(y_test, y_prob), 4),
                **perf
            })

        # =========================================================
        # 4. èµ„äº§æŒä¹…åŒ– (ä½ç½®ï¼šç»“å±€å¾ªç¯å†…ï¼Œæ¨¡å‹å¾ªç¯å¤–)
        # =========================================================
        # åŠŸèƒ½ 4.1: ä¿å­˜é˜ˆå€¼ JSON (ä¾› eICU å¤–éƒ¨éªŒè¯ç›´æ¥è°ƒç”¨)
        th_json_path = os.path.join(target_dir, "thresholds.json")
        with open(th_json_path, 'w') as f:
            json.dump(target_thresholds, f, indent=4)
        
        # åŠŸèƒ½ 4.2: ä¿å­˜æ•ˆèƒ½æŠ¥å‘Š (Table 3 å†…å®¹)
        perf_df = pd.DataFrame(target_perf_report)
        # å­˜å…¥æ¨¡å‹å­ç›®å½•
        perf_df.to_csv(os.path.join(target_dir, "internal_diagnostic_perf.csv"), index=False)
        # å­˜å…¥å…¨å±€ Table æ±‡æ€»ç›®å½•
        perf_df.to_csv(os.path.join(TABLE_ROOT, f"Table3_Internal_Perf_{target}.csv"), index=False)
        
        print(f"  âœ… é˜ˆå€¼èµ„äº§å·²ç»‘å®š: {th_json_path}")
        best_model_info = max(target_perf_report, key=lambda x: x['F1_Score'])
        print(f"  âœ… å®¡è®¡å®Œæˆã€‚æœ€ä¼˜ç®—æ³•: {best_model_info['Algorithm']} (F1: {best_model_info['F1_Score']})")

    # 5. ä¿å­˜å…¨ç»“å±€å…¨å±€æ±‡æ€»è¡¨ (ä½ç½®ï¼šæ‰€æœ‰å¾ªç¯ç»“æŸå)
    if global_summary:
        summary_df = pd.DataFrame(global_summary)
        summary_df.to_csv(os.path.join(MODEL_ROOT, "global_diagnostic_summary.csv"), index=False)
        print(f"\nğŸ“Š ä»»åŠ¡åœ†æ»¡å®Œæˆï¼å…¨å±€æ±‡æ€»æŠ¥å‘Šå·²ç”Ÿæˆ: {MODEL_ROOT}/global_diagnostic_summary.csv")

if __name__ == "__main__":
    run_cutoff_optimization_flow()
