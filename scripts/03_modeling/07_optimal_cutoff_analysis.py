import os
import json
import joblib
import re
import matplotlib.patches as mpatches
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, confusion_matrix, f1_score, roc_auc_score

# =========================================================
# 1. åŸºç¡€é…ç½®
# =========================================================
BASE_DIR = "../../"
MODEL_ROOT = os.path.join(BASE_DIR, "artifacts/models")
FIG_ROOT = os.path.join(BASE_DIR, "results/figures")
TABLE_ROOT = os.path.join(BASE_DIR, "results/tables")
OUTCOMES = ['pof', 'mortality', 'composite']

for path in [FIG_ROOT, TABLE_ROOT]:
    if not os.path.exists(path):
        os.makedirs(path)

def calculate_detailed_metrics(y_true, y_prob, threshold):
    """åŸºäºç‰¹å®šé˜ˆå€¼è®¡ç®—ä¸´åºŠè¯Šæ–­æŒ‡æ ‡"""
    y_pred = (y_prob >= threshold).astype(int)
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
    
    # è®¡ç®—ç‚¹ä¼°è®¡
    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
    
    metrics = {
        "Threshold": round(threshold, 4),
        "TP": int(tp), "FP": int(fp), "TN": int(tn), "FN": int(fn),
        "Sensitivity": round(sensitivity, 4),
        "Specificity": round(specificity, 4),
        "PPV": round(tp / (tp + fp), 4) if (tp + fp) > 0 else 0,
        "NPV": round(tn / (tn + fn), 4) if (tn + fn) > 0 else 0,
        "F1_Score": round(f1_score(y_true, y_pred), 4),
        "Accuracy": round((tp + tn) / (tp + tn + fp + fn), 4)
        # "Sen_CI": "N/A"  # å¦‚æœä¸è·‘ Bootstrapï¼Œå»ºè®®å…ˆæ³¨é‡Šæ‰æˆ–è®¾ä¸º N/A
    }
    return metrics

def plot_diagnostic_viz(y_true, y_prob, threshold, name, target, save_dir):
    """
    ç”Ÿæˆç§‘ç ”çº§è¯Šæ–­æ•ˆèƒ½å›¾ï¼š
    """
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    sns.set_context("paper", font_scale=1.2)
    plt.rcParams['font.sans-serif'] = ['Arial']
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6), dpi=300)
    c_normal = "#4DBBD5FF" # è“è‰²
    c_event = "#E64B35FF"  # çº¢è‰²
    c_main = "#3C3C3CFF"   # æ·±ç°
    fpr, tpr, _ = roc_curve(y_true, y_prob)
    auc_val = roc_auc_score(y_true, y_prob)
    auc_label = f'Model AUC: {auc_val:.3f}'
    ci_path = os.path.join(MODEL_ROOT, target, "bootstrap_ci_stats.pkl")
    if os.path.exists(ci_path):
        try:
            boot_stats = joblib.load(ci_path)
            if name in boot_stats:
                auc_label = f"{name} AUC: {boot_stats[name]['auc_ci']}"
        except: pass

    ax1.plot(fpr, tpr, label=auc_label, color=c_main, lw=2.5)
    ax1.plot([0, 1], [0, 1], linestyle='--', color='gray', alpha=0.5, lw=1)
    
    ax1.text(0.6, 0.1, f'AUC = {auc_val:.3f}', fontsize=12, 
             fontweight='bold', color=c_main, bbox=dict(facecolor='white', alpha=0.8, edgecolor='none'))
    perf = calculate_detailed_metrics(y_true, y_prob, threshold)
    ax1.scatter(1-perf['Specificity'], perf['Sensitivity'], 
                color=c_event, s=120, edgecolors='white', zorder=5,
                label=f'Optimal Cutoff: {threshold:.3f}')
    ax1.annotate(f'Sensitivity: {perf["Sensitivity"]:.2f}\nSpecificity: {perf["Specificity"]:.2f}',
                 xy=(1-perf['Specificity'], perf['Sensitivity']), 
                 xytext=(1-perf['Specificity']+0.12, perf['Sensitivity']-0.15),
                 fontsize=10, fontweight='bold',
                 arrowprops=dict(arrowstyle="->", color='black', connectionstyle="arc3,rad=.2"))
    ax1.set_xlabel('1 - Specificity (False Positive Rate)', labelpad=10)
    ax1.set_ylabel('Sensitivity (True Positive Rate)', labelpad=10)
    ax1.set_title(f'Diagnostic Performance: {name}\n({target.upper()})', fontweight='bold', pad=15)
    ax1.legend(loc='lower right', frameon=False)
    ax1.set_aspect('equal') # ä¿æŒ ROC æ­£æ–¹å½¢
    df_prob = pd.DataFrame({'prob': y_prob, 'target': y_true})
    sns.kdeplot(data=df_prob[df_prob['target'] == 0], x='prob', fill=True, 
                label='Normal/Survival', color=c_normal, ax=ax2, alpha=0.4, lw=2)
    sns.kdeplot(data=df_prob[df_prob['target'] == 1], x='prob', fill=True, 
                label='Outcome Event', color=c_event, ax=ax2, alpha=0.4, lw=2)
    ax2.axvline(threshold, color=c_main, linestyle='--', lw=2, alpha=0.8)
    ylim = ax2.get_ylim()[1]
    text_style = dict(fontsize=10, fontweight='bold', bbox=dict(facecolor='white', alpha=0.6, edgecolor='none', pad=1))
    
    ax2.text(threshold-0.03, ylim*0.92, 'LOW RISK', ha='right', color=c_normal, **text_style)
    ax2.text(threshold+0.03, ylim*0.85, 'HIGH RISK', ha='left', color=c_event, **text_style)
    ax2.set_xlabel('Predicted Risk Probability', labelpad=10)
    ax2.set_ylabel('Population Density', labelpad=10)
    ax2.set_title('Clinical Risk Stratification', fontweight='bold', pad=15)
    ax2.legend(frameon=False)
    sns.despine() # ç§»é™¤ä¸Šæ–¹å’Œå³ä¾§è¾¹æ¡†
    plt.tight_layout()
    save_filename = f"07_Diagnostic_{name}_{target}"
    save_base = os.path.join(save_dir, save_filename) # æ•´åˆè·¯å¾„å®šä¹‰
    plt.savefig(os.path.join(save_dir, f"{save_filename}.png"), bbox_inches='tight')
    plt.savefig(os.path.join(save_dir, f"{save_filename}.pdf"), bbox_inches='tight')
    plt.savefig(f"{save_base}.png", bbox_inches='tight')
    plt.savefig(f"{save_base}.pdf", bbox_inches='tight')
    plt.close()
    print(f"  âœ… å‡ºç‰ˆçº§è¯Šæ–­å›¾å·²ç”Ÿæˆ: {save_base}")

def export_formatted_table3():
    summary_path = os.path.join(MODEL_ROOT, "global_diagnostic_summary.csv")
    if not os.path.exists(summary_path):
        print("âš ï¸ æœªæ‰¾åˆ°æ±‡æ€»è¡¨ï¼Œæ— æ³•å¯¼å‡º Table 3"); return
    
    df = pd.read_csv(summary_path)
    formatted_rows = []
    ci_cache = {}
    
    for _, row in df.iterrows():
        target = str(row['Outcome']).lower()
        algo = row['Algorithm']
        group_raw = str(row['Group'])
        
        # 1. åŠ è½½ç¬¬ 06 æ­¥ CI èµ„äº§
        if target not in ci_cache:
            ci_path = os.path.join(MODEL_ROOT, target, "bootstrap_ci_stats.pkl")
            ci_cache[target] = joblib.load(ci_path) if os.path.exists(ci_path) else None

        # 2. äººç¾¤åˆ†ç±»ä¸ CI ç´¢å¼•åŒ¹é…
        if 'Full' in group_raw:
            display_group, ci_key, group_order = 'Full Population', 'main', 0
        else:
            display_group, ci_key, group_order = 'Subgroup (No Renal)', 'sub', 1

        # 3. æ ¼å¼åŒ– AUC (95% CI)
        auc_val = row['AUC']
        auc_str = f"{auc_val:.3f}"
        target_ci = ci_cache.get(target)
        if target_ci and algo in target_ci:
            try:
                low, high = target_ci[algo][ci_key]
                auc_str = f"{auc_val:.3f} ({low:.3f}â€“{high:.3f})"
            except (KeyError, TypeError):
                pass 

        # 4. æ„é€ æ ‡å‡†è¡Œ
        formatted_rows.append({
            'Outcome': row['Outcome'].upper(),
            'Group': display_group,
            'Model': algo,
            'AUC (95% CI)': auc_str,
            'Sens.': f"{row['Sensitivity']:.3f}",
            'Spec.': f"{row['Specificity']:.3f}",
            'F1': f"{row['F1_Score']:.3f}",
            'Optimal Cut-off': f"{row['Threshold']:.3f}",
            'auc_numeric': auc_val,
            'group_priority': group_order
        })

    # 5. å¤šçº§é€»è¾‘æ’åºå¹¶å¯¼å‡º
    table3 = pd.DataFrame(formatted_rows)
    table3 = table3.sort_values(
        ['Outcome', 'group_priority', 'auc_numeric'], 
        ascending=[True, True, False]
    )

    final_columns = ['Outcome', 'Group', 'Model', 'AUC (95% CI)', 'Sens.', 'Spec.', 'F1', 'Optimal Cut-off']
    output_path = os.path.join(TABLE_ROOT, "Table3_Final_Performance_SCI.csv")
    table3[final_columns].to_csv(output_path, index=False, encoding='utf-8-sig')
    print(f"âœ¨ Table 3 (Full vs Subgroup) å·²å¯¼å‡ºè‡³: {output_path}")
    
class ResultVisualizer:
    def __init__(self, base_dir="../../"):
        self.model_root = os.path.join(base_dir, "artifacts/models")
        self.result_root = os.path.join(base_dir, "results/figures")
        self.report_path = os.path.join(self.model_root, "performance_report.csv")
        os.makedirs(self.result_root, exist_ok=True)
        
        # åŒ»å­¦æœŸåˆŠçº§å…¨å±€è®¾ç½®
        plt.rcParams.update({
            'font.family': 'sans-serif',
            'font.sans-serif': ['Arial', 'Helvetica'],
            'axes.labelweight': 'bold',
            'axes.titlesize': 14,
            'axes.labelsize': 11,
            'xtick.labelsize': 10,
            'ytick.labelsize': 10,
            'legend.fontsize': 10,
            'axes.linewidth': 1.2,
            'pdf.fonttype': 42, # ç¡®ä¿æ–‡å­—å¯ç¼–è¾‘
            'ps.fonttype': 42
        })
        
        # å®šä¹‰ç§‘å­¦å‡ºç‰ˆé…è‰² (NPG é£æ ¼)
        self.sci_palette = ["#E64B35FF", "#4DBBD5FF", "#00A087FF", "#3C3C3CFF", "#F39B7FFF", "#8491B4FF"]

    def summarize_feature_importance(self, outcomes=['pof', 'mortality', 'composite'], top_n=15):
        """åŒ»å­¦å‡ºç‰ˆçº§ç‰¹å¾é‡è¦æ€§æ±‡æ€»å›¾"""
        print("ğŸ“Š æ­£åœ¨ç”Ÿæˆå‡ºç‰ˆçº§ç‰¹å¾é‡è¦æ€§å›¾...")
        all_imps = []
        for target in outcomes:
            path = os.path.join(self.model_root, target, "feature_importance.csv")
            if os.path.exists(path):
                all_imps.append(pd.read_csv(path))
        
        if not all_imps: return

        full_df = pd.concat(all_imps, ignore_index=True)
        pivot_df = full_df.groupby(['feature', 'outcome'])['importance'].mean().unstack()
        pivot_df['Global_Avg'] = pivot_df.mean(axis=1)
        top_feats = pivot_df.sort_values('Global_Avg', ascending=False).head(top_n).index.tolist()

        # å¼€å§‹ç»˜å›¾
        fig, ax = plt.subplots(figsize=(10, 8), dpi=300)
        plot_data = full_df[full_df['feature'].isin(top_feats)]
        
        # ä½¿ç”¨æ›´ç²¾è‡´çš„æ¡å½¢å›¾
        sns.barplot(data=plot_data, y='feature', x='importance', hue='outcome', 
                    order=top_feats, palette="mako", alpha=0.9, edgecolor="white", linewidth=0.5)
        
        # æ·»åŠ è½»é‡çº§å‚ç›´ç½‘æ ¼çº¿
        ax.xaxis.grid(True, linestyle='--', alpha=0.4, color='#CCCCCC')
        ax.set_axisbelow(True)

        plt.title("Primary Clinical Predictors of Outcomes", loc='left', pad=20)
        plt.xlabel("Mean Relative Feature Importance (Normalized)")
        plt.ylabel("")
        
        # ä¼˜åŒ–å›¾ä¾‹
        plt.legend(title="Clinical Outcome", frameon=False, loc='lower right', bbox_to_anchor=(1, 0.05))
        
        sns.despine()
        plt.tight_layout()
        
        save_path = os.path.join(self.result_root, "sci_feature_importance")
        plt.savefig(f"{save_path}.pdf", bbox_inches='tight')
        plt.savefig(f"{save_path}.png", dpi=600, bbox_inches='tight')
        plt.close()

    def plot_performance_forest(self):
        """åŒ»å­¦å‡ºç‰ˆçº§æ£®æ—å›¾ï¼ˆæ›´æ¸…æ™°çš„åˆ†ç»„ä¸å¯¹é½ï¼‰"""
        print("ğŸŒ² æ­£åœ¨ç”Ÿæˆå‡ºç‰ˆçº§æ£®æ—å›¾...")
        if not os.path.exists(self.report_path): return

        df = pd.read_csv(self.report_path)
        def parse_ci(s):
            vals = re.findall(r"([0-9.]+)", str(s))
            return [float(x) for x in vals[:3]] if len(vals) >= 3 else [np.nan]*3
        parsed = df['Main CI'].apply(parse_ci).tolist()
        df[['auc', 'low', 'high']] = pd.DataFrame(parsed, index=df.index)

        outcomes = df['Outcome'].unique()
        models = df['Algorithm'].unique()
        color_map = dict(zip(models, self.sci_palette))

        fig, ax = plt.subplots(figsize=(11, 8), dpi=300)
        
        y_pos = 0
        y_ticks, y_labels = [], []

        for outcome in reversed(outcomes):
            sub_df = df[df['Outcome'] == outcome]
            
            # ç»“å±€åˆ†ç»„æ¨ªå¸¦ (åŒ»å­¦æœŸåˆŠå¸¸ç”¨é£æ ¼)
            y_pos += 1
            ax.axhspan(y_pos-0.5, y_pos+len(sub_df)+0.5, color='#F8F9FA', alpha=0.8, zorder=0)
            
            # åˆ†ç»„æ ‡é¢˜
            ax.text(0.51, y_pos + (len(sub_df)/2) + 0.5, f"Outcome: {outcome.upper()}", 
                    va='center', ha='left', fontweight='black', fontsize=12, 
                    color='#2C3E50', style='italic')
            
            for _, row in sub_df.iterrows():
                y_pos += 1
                y_ticks.append(y_pos)
                y_labels.append(row['Algorithm'])
                
                # ç»˜åˆ¶ç½®ä¿¡åŒºé—´çº¿æ¡
                ax.plot([row['low'], row['high']], [y_pos, y_pos], color=color_map[row['Algorithm']], 
                        linewidth=2.5, solid_capstyle='round', zorder=3)
                
                # ç»˜åˆ¶ä¸­å¿ƒç‚¹ (Marker)
                ax.scatter(row['auc'], y_pos, color=color_map[row['Algorithm']], 
                           s=80, edgecolors='white', linewidth=1, zorder=4)
                
                # æ•°å€¼æ ‡æ³¨ (å¯¹é½å¯¹é½)
                label_text = f"{row['auc']:.3f} [{row['low']:.3f} - {row['high']:.3f}]"
                ax.text(1.01, y_pos, label_text, va='center', ha='left', fontsize=9.5, fontfamily='monospace')

            y_pos += 1.5 # ç»„é—´è·

        # å›¾è¡¨ä¿®é¥°
        ax.set_yticks(y_ticks)
        ax.set_yticklabels(y_labels, fontweight='normal')
        ax.axvline(0.5, color='#34495E', linestyle='-', linewidth=1.5, alpha=0.8, label='Chance level')
        ax.axvline(0.8, color='#BDC3C7', linestyle='--', linewidth=0.8, alpha=0.5)
        
        ax.set_xlabel('Area Under the ROC Curve (95% CI)', labelpad=15)
        ax.set_xlim(0.5, 1.0) # AUC ä¸å¯èƒ½è¶…è¿‡ 1.0
        
        # ç§»é™¤åæ ‡è½´å†—ä½™
        ax.spines['top'].set_visible(False)
        ax.spines['right'].set_visible(False)
        ax.spines['left'].set_visible(False)
        ax.xaxis.set_ticks_position('bottom')
        
        plt.title("Predictive Performance Across Clinical Outcomes", loc='left', pad=30, fontsize=16)
        
        # å›¾ä¾‹ç¾åŒ–
        patches = [mpatches.Patch(color=color_map[m], label=m) for m in models]
        ax.legend(handles=patches, loc='lower center', bbox_to_anchor=(0.5, -0.15), 
                  ncol=len(models), frameon=False, fontsize=9)

        plt.tight_layout()
        save_path = os.path.join(self.result_root, "sci_forest_plot")
        plt.savefig(f"{save_path}.pdf", bbox_inches='tight', transparent=True)
        plt.savefig(f"{save_path}.png", dpi=600, bbox_inches='tight')
        plt.close()
        print("âœ… å‡ºç‰ˆçº§å›¾è¡¨å·²ç”Ÿæˆã€‚")

# 2. æ ¸å¿ƒæ‰§è¡Œé€»è¾‘
def run_cutoff_optimization_flow():
    print("ğŸš€ å¯åŠ¨ 07 æ­¥ï¼šè‡ªåŠ¨åŒ–é˜ˆå€¼å¯»ä¼˜ä¸ä¸´åºŠæ•ˆèƒ½å®¡è®¡...")
    global_summary = []

    for target in OUTCOMES:
        target_dir = os.path.join(MODEL_ROOT, target)
        fig_save_dir = os.path.join(FIG_ROOT, target)
        
        if not os.path.exists(target_dir):
            print(f"âš ï¸ è·³è¿‡ {target}: è·¯å¾„ç¼ºå¤±"); continue

        print(f"\n--- æ­£åœ¨å¤„ç†ç»ˆç‚¹: [{target.upper()}] ---")

        try:
            # 1. èµ„äº§åŠ è½½ä¸ç‰¹å¾å¯¹é½
            models_dict = joblib.load(os.path.join(target_dir, "all_models_dict.pkl"))
            eval_data = joblib.load(os.path.join(target_dir, "eval_data.pkl"))
            X_test_pre, y_test = eval_data['X_test_pre'], eval_data['y_test']
            feat_path = os.path.join(target_dir, "selected_features.json")
            if os.path.exists(feat_path):
                with open(feat_path, 'r') as f:
                    feat_data = json.load(f)
                
                # å…³é”®ä¿®å¤ï¼šä»å­—å…¸ä¸­æå– "features" åˆ—è¡¨
                if isinstance(feat_data, dict) and "features" in feat_data:
                    selected_features = feat_data["features"]
                else:
                    selected_features = list(feat_data) # å…œåº•é€»è¾‘
                
                # ä»…é€‰æ‹©æµ‹è¯•é›†ä¸­å­˜åœ¨çš„ç‰¹å¾
                valid_cols = [c for c in selected_features if c in X_test_pre.columns]
                X_eval = X_test_pre[valid_cols].values
            else:
                X_eval = X_test_pre.values

            best_model_name = max(
                models_dict.keys(), 
                key=lambda n: roc_auc_score(y_test, models_dict[n].predict_proba(X_eval)[:, 1])
            )
            print(f"  ğŸ† é€‰å®šæœ€ä½³æ¨¡å‹è¿›è¡Œè¯Šæ–­å¯è§†åŒ–: {best_model_name}")

        except Exception as e:
            print(f"âŒ èµ„äº§è§£æå¤±è´¥ ({target}): {e}")
            continue

        target_thresholds, target_perf_report = {}, []

        # 2. éå†æ¨¡å‹ï¼šè®¡ç®—é˜ˆå€¼ä¸å¤šç»´æ•ˆèƒ½
        for name, clf in models_dict.items():
            y_prob = clf.predict_proba(X_eval)[:, 1]
            fpr, tpr, thresholds = roc_curve(y_test, y_prob)
            auc_val = roc_auc_score(y_test, y_prob)
            
            # Youden Index å¯»ä¼˜ (å…¨äººç¾¤)
            if len(thresholds) <= 1:
                best_th = 0.5
            else:
                youden_index = tpr + (1 - fpr) - 1
                best_th = float(thresholds[np.argmax(youden_index)])
            
            best_th = min(1.0, best_th) # ä¿®æ­£éæ³•é˜ˆå€¼
            target_thresholds[name] = best_th

            # --- å…¨äººç¾¤æ•ˆèƒ½å®¡è®¡ ---
            perf_main = calculate_detailed_metrics(y_test, y_prob, best_th)
            perf_main.update({
                'Algorithm': name, 
                'Group': 'Full Population', 
                'Outcome': target,
                'AUC': round(auc_val, 4) # ã€æ–°å¢ã€‘åŠ å…¥ AUC
            })
            target_perf_report.append(perf_main)

            # --- äºšç»„æ•ˆèƒ½å®¡è®¡ (è‚¾ç—…äºšç»„ä¿æŠ¤é€»è¾‘) ---
            if 'sub_mask' in eval_data:
                mask = eval_data['sub_mask']
                # ã€æ–°å¢ï¼šæ ·æœ¬é‡ä¿æŠ¤æ£€æŸ¥ã€‘
                if mask.sum() > 10 and len(np.unique(y_test[mask])) > 1:
                    y_prob_sub = y_prob[mask]
                    y_test_sub = y_test[mask]
                    
                    # ã€ä¼˜åŒ–ï¼šè®¡ç®—äºšç»„ç‹¬ç«‹æœ€ä¼˜æˆªæ–­å€¼ã€‘
                    fpr_s, tpr_s, th_s = roc_curve(y_test_sub, y_prob_sub)
                    youden_s = tpr_s + (1 - fpr_s) - 1
                    best_th_sub = float(th_s[np.argmax(youden_s)])
                    
                    # ä½¿ç”¨ä¸»äººç¾¤é˜ˆå€¼è¯„ä¼°å½“å‰æ€§èƒ½
                    perf_sub = calculate_detailed_metrics(y_test_sub, y_prob_sub, best_th)
                    perf_sub.update({
                        'Algorithm': name, 
                        'Group': 'Subgroup (Non-Renal)', 
                        'Outcome': target,
                        'AUC': round(roc_auc_score(y_test_sub, y_prob_sub), 4),
                        'Subgroup_Specific_Th': round(best_th_sub, 4) # å­˜å‚¨ç‹¬ç«‹çš„å»ºè®®é˜ˆå€¼
                    })
                    target_perf_report.append(perf_sub)

            # ä»…ä¸ºæœ€ä½³æ¨¡å‹ç”Ÿæˆè¯Šæ–­å¯è§†åŒ–å›¾ï¼Œé¿å…å›¾ç‰‡å†—ä½™
            if name == best_model_name:
                plot_diagnostic_viz(y_test, y_prob, best_th, name, target, fig_save_dir)

        # 3. èµ„äº§æŒä¹…åŒ–
        # ä¿å­˜é˜ˆå€¼ JSON (ç”¨äº eICU å¤–éƒ¨éªŒè¯ä¸€é”®æ˜ å°„)
        with open(os.path.join(target_dir, "thresholds.json"), 'w') as f:
            json.dump(target_thresholds, f, indent=4)
        
        # å­˜å…¥ç»“å±€å­ç›®å½•ä¸ Table æ±‡æ€»ç›®å½•
        perf_df = pd.DataFrame(target_perf_report)
        perf_df.to_csv(os.path.join(target_dir, "internal_diagnostic_perf.csv"), index=False)
        perf_df.to_csv(os.path.join(TABLE_ROOT, f"Table3_Perf_{target}.csv"), index=False)
        
        global_summary.extend(target_perf_report)
        
        # å®æ—¶åé¦ˆ
        best_perf = next(p for p in target_perf_report if p['Algorithm'] == best_model_name and p['Group'] == 'Full Population')
        print(f"  âœ… å®¡è®¡å®Œæˆã€‚æœ€ä¼˜æ¨¡å‹ F1: {best_perf['F1_Score']} (AUC: {best_perf['AUC']})")

    # 4. å…¨å±€æ±‡æ€»å¹¶æŒ‰å­¦æœ¯é€»è¾‘æ’åº
    if global_summary:
        summary_df = pd.DataFrame(global_summary)
        # æ’åºï¼šç»“å±€å‡åº -> åˆ†ç»„å‡åº -> AUC é™åº
        summary_df = summary_df.sort_values(['Outcome', 'Group', 'AUC'], ascending=[True, True, False])
        summary_df.to_csv(os.path.join(MODEL_ROOT, "global_diagnostic_summary.csv"), index=False)
        print(f"\nğŸ“Š ä»»åŠ¡åœ†æ»¡å®Œæˆï¼å…¨å±€æŠ¥å‘Šè§: {MODEL_ROOT}/global_diagnostic_summary.csv")

if __name__ == "__main__":
    # 1. æ‰§è¡Œ 07 æ­¥ä¸»æµç¨‹ï¼šè®¡ç®—é˜ˆå€¼ä¸æ•ˆèƒ½
    run_cutoff_optimization_flow()
    
    # 2. è‡ªåŠ¨å¯¼å‡º Table 3 (Excel æˆ– CSV)
    export_formatted_table3()
    
    # 3. å®ä¾‹åŒ–å¯è§†åŒ–å·¥å…·
    viz = ResultVisualizer(base_dir=BASE_DIR)
    
    # 4. ç”Ÿæˆç‰¹å¾é‡è¦æ€§æ±‡æ€»å›¾ (åŸºäºç¬¬ 7 æ­¥åçš„èµ„äº§)
    viz.summarize_feature_importance(outcomes=OUTCOMES, top_n=15)
    
    # 5. ã€å…³é”®ä¿®å¤ã€‘ç”Ÿæˆæ£®æ—å›¾ï¼šå¼ºåˆ¶ä½¿ç”¨ç¬¬ 06 æ­¥ç”Ÿæˆçš„ç½®ä¿¡åŒºé—´æŠ¥å‘Š
    step6_report_path = os.path.join(MODEL_ROOT, "performance_report.csv")
    
    if os.path.exists(step6_report_path):
        print(f"ğŸŒ² æ­£åœ¨åŸºäºç¬¬ 06 æ­¥çš„ç½®ä¿¡åŒºé—´æ•°æ®ç”Ÿæˆæ£®æ—å›¾: {step6_report_path}")
        viz.report_path = step6_report_path
        viz.plot_performance_forest()
    else:
        print("âš ï¸ è­¦å‘Š: æœªæ‰¾åˆ°ç¬¬ 06 æ­¥çš„ performance_report.csvã€‚")
        print("è¯·ç¡®ä¿å·²è¿è¡Œç¬¬ 06 æ­¥è„šæœ¬ï¼Œæ£®æ—å›¾éœ€è¦å…¶æä¾›çš„ 95% CI æ•°æ®ã€‚")
