import os
import json
import joblib
import numpy as np
import pandas as pd
import optuna
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.calibration import CalibratedClassifierCV, calibration_curve
from sklearn.metrics import roc_auc_score, brier_score_loss, roc_curve
from sklearn.utils import resample
import warnings

# åŸºç¡€é…ç½®
warnings.filterwarnings('ignore')
optuna.logging.set_verbosity(optuna.logging.WARNING)

# è·¯å¾„ç®¡ç†
BASE_DIR = "../../"
INPUT_PATH = os.path.join(BASE_DIR, "data/cleaned/mimic_processed.csv")
JSON_FEAT_PATH = os.path.join(BASE_DIR, "artifacts/features/selected_features.json")
SAVE_DIR = os.path.join(BASE_DIR, "artifacts/models")
FIG_DIR = os.path.join(BASE_DIR, "results/figures/performance")

for d in [SAVE_DIR, FIG_DIR]:
    os.makedirs(d, exist_ok=True)

# =========================================================
# è¾…åŠ©å·¥å…·å‡½æ•°
# =========================================================
def get_auc_ci(model, X_test, y_test, n_bootstraps=1000):
    """è®¡ç®— AUC çš„ 95% ç½®ä¿¡åŒºé—´ (Bootstrap)"""
    scores = []
    indices = np.arange(len(y_test))
    for i in range(n_bootstraps):
        resample_idx = resample(indices, random_state=i)
        y_res = y_test.iloc[resample_idx]
        if len(np.unique(y_res)) < 2: continue
        prob = model.predict_proba(X_test[resample_idx])[:, 1]
        scores.append(roc_auc_score(y_res, prob))
    sorted_scores = np.sort(scores)
    if len(sorted_scores) == 0: return 0, 0
    return sorted_scores[int(0.025 * len(sorted_scores))], sorted_scores[int(0.975 * len(sorted_scores))]

# =========================================================
# æ ¸å¿ƒè®­ç»ƒæµæ°´çº¿
# =========================================================
def run_model_training_flow():
    # 1. åŠ è½½æ•°æ®ä¸ç‰¹å¾é…ç½®
    df = pd.read_csv(INPUT_PATH)
    with open(JSON_FEAT_PATH, 'r') as f:
        feature_config = json.load(f)
    
    global_performance = []

    for target in feature_config.keys():
        print(f"\n\n{'='*30} æ­£åœ¨åˆ†æç»“å±€: {target.upper()} {'='*30}")
        
        # 2. å‡†å¤‡è¯¥ç»“å±€ä¸“å±ç‰¹å¾é›†
        selected_features = feature_config[target]['features']
        X = df[selected_features].copy()
        y = df[target]
        subgroup = df['subgroup_no_renal']

        # 3. åˆ’åˆ†ä¸é¢„å¤„ç†
        X_train, X_test, y_train, y_test, _, sub_test = train_test_split(
            X, y, subgroup, test_size=0.2, random_state=42, stratify=y
        )

        imputer = IterativeImputer(max_iter=10, random_state=42)
        scaler = StandardScaler()
        
        X_train_imp = scaler.fit_transform(imputer.fit_transform(X_train))
        X_test_imp = scaler.transform(imputer.transform(X_test))

        # 4. XGBoost Optuna è´å¶æ–¯å¯»ä¼˜
        print(f"ğŸ”¬ å¯åŠ¨ {target} çš„ XGBoost å¯»ä¼˜...")
        def objective(trial):
            param = {
                'n_estimators': trial.suggest_int('n_estimators', 100, 500),
                'max_depth': trial.suggest_int('max_depth', 3, 6),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),
                'subsample': trial.suggest_float('subsample', 0.6, 0.9),
                'random_state': 42, 'eval_metric': 'logloss'
            }
            model = XGBClassifier(**param)
            cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
            return cross_val_score(model, X_train_imp, y_train, cv=cv, scoring='roc_auc').mean()

        study = optuna.create_study(direction='maximize')
        study.optimize(objective, n_trials=30) # ç”Ÿäº§ç¯å¢ƒå»ºè®® 50-100
        best_xgb = XGBClassifier(**study.best_params, random_state=42)

        # 5. æ¨¡å‹ç«èµ› (å«æ¦‚ç‡æ ¡å‡†)
        models = {
            "Logistic Regression": LogisticRegression(class_weight='balanced', max_iter=1000),
            "Random Forest": RandomForestClassifier(n_estimators=200, class_weight='balanced', random_state=42),
            "XGBoost": best_xgb,
            "SVM": SVC(probability=True, class_weight='balanced'),
            "Decision Tree": DecisionTreeClassifier(max_depth=5, class_weight='balanced')
        }

        calibrated_results = {}
        target_summary = []

        print("\n" + "-"*95)
        print(f"{'Algorithm':<20} | {'Main AUC (95% CI)':<30} | {'Brier':<10}")
        print("-" * 95)

        for name, m in models.items():
            # æ¦‚ç‡æ ¡å‡†
            clf = CalibratedClassifierCV(m, cv=3, method='isotonic')
            clf.fit(X_train_imp, y_train)
            calibrated_results[name] = clf

            # å…¨äººç¾¤è¯„ä¼°
            y_prob = clf.predict_proba(X_test_imp)[:, 1]
            auc_main = roc_auc_score(y_test, y_prob)
            brier = brier_score_loss(y_test, y_prob)
            low_m, high_m = get_auc_ci(clf, X_test_imp, y_test)
            
            # äºšç»„è¯„ä¼° (No-Renal)
            sub_mask = (sub_test == 1).values
            auc_sub = roc_auc_score(y_test[sub_mask], y_prob[sub_mask])
            low_s, high_s = get_auc_ci(clf, X_test_imp[sub_mask], y_test[sub_mask])

            main_auc_str = f"{auc_main:.3f} ({low_m:.3f}-{high_m:.3f})"
            sub_auc_str = f"{auc_sub:.3f} ({low_s:.3f}-{high_s:.3f})"
            
            print(f"{name:<20} | {main_auc_str:<30} | {brier:.4f}")

            target_summary.append({
                "Outcome": target,
                "Algorithm": name,
                "Main AUC": round(auc_main, 4),
                "Main AUC CI": main_auc_str,
                "No-Renal AUC CI": sub_auc_str,
                "Brier": round(brier, 4)
            })

        # 6. ä¿å­˜èµ„äº§
        joblib.dump(calibrated_results, os.path.join(SAVE_DIR, f"models_{target}.pkl"))
        global_performance.extend(target_summary)

        # 7. ç»˜å›¾
        plot_performance(calibrated_results, X_test_imp, y_test, target)

    # ç”Ÿæˆæœ€ç»ˆæ±‡æ€»è¡¨
    pd.DataFrame(global_performance).to_csv(os.path.join(SAVE_DIR, "performance_report.csv"), index=False)
    print(f"\nğŸš€ è®­ç»ƒæµç¨‹å…¨éƒ¨å®Œæˆï¼æŠ¥å‘Šå·²å­˜è‡³: {SAVE_DIR}")

def plot_performance(models, X_test, y_test, target):
    """å°† ROC å’Œ Calibration æ›²çº¿ç”Ÿæˆä¸ºä¸¤ä¸ªç‹¬ç«‹çš„å­¦æœ¯å›¾ç‰‡"""
    
    # --- 1. ç»˜åˆ¶å¹¶ä¿å­˜ç‹¬ç«‹ ROC æ›²çº¿ ---
    plt.figure(figsize=(8, 7), dpi=300) # é«˜æ¸…åˆ†è¾¨ç‡
    for name, clf in models.items():
        y_prob = clf.predict_proba(X_test)[:, 1]
        fpr, tpr, _ = roc_curve(y_test, y_prob)
        auc_val = roc_auc_score(y_test, y_prob)
        plt.plot(fpr, tpr, lw=2, label=f"{name} (AUC = {auc_val:.3f})")
    
    plt.plot([0, 1], [0, 1], color='gray', linestyle='--', lw=1.5, alpha=0.7)
    plt.xlim([-0.02, 1.02])
    plt.ylim([-0.02, 1.02])
    plt.xlabel("False Positive Rate (1 - Specificity)", fontsize=12)
    plt.ylabel("True Positive Rate (Sensitivity)", fontsize=12)
    plt.title(f"ROC Curves: Predicted {target.upper()}", fontsize=14, fontweight='bold')
    plt.legend(loc="lower right", fontsize=10)
    plt.grid(alpha=0.3)
    plt.tight_layout()
    
    roc_path = os.path.join(FIG_DIR, f"Figure_ROC_{target}.png")
    plt.savefig(roc_path, bbox_inches='tight')
    plt.show()
    plt.close()

    # --- 2. ç»˜åˆ¶å¹¶ä¿å­˜ç‹¬ç«‹æ ¡å‡†æ›²çº¿ (Calibration) ---
    plt.figure(figsize=(8, 7), dpi=300)
    for name, clf in models.items():
        y_prob = clf.predict_proba(X_test)[:, 1]
        prob_true, prob_pred = calibration_curve(y_test, y_prob, n_bins=10)
        plt.plot(prob_pred, prob_true, marker='o', lw=2, ms=6, label=name)
    
    plt.plot([0, 1], [0, 1], color='gray', linestyle='--', lw=1.5, label='Perfectly Calibrated')
    plt.xlabel("Predicted Probability", fontsize=12)
    plt.ylabel("Actual Probability", fontsize=12)
    plt.title(f"Calibration Curves: Predicted {target.upper()}", fontsize=14, fontweight='bold')
    plt.legend(loc="upper left", fontsize=10)
    plt.grid(alpha=0.3)
    plt.tight_layout()
    
    calib_path = os.path.join(FIG_DIR, f"Figure_Calibration_{target}.png")
    plt.savefig(calib_path, bbox_inches='tight')
    plt.show()
    plt.close()
    
    print(f"âœ… å›¾ç‰‡å·²ä¿å­˜:\n   - ROC: {roc_path}\n   - Calib: {calib_path}")

if __name__ == "__main__":
    run_model_training_flow()
