import os
import json
import joblib
import numpy as np
import pandas as pd
import optuna
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.calibration import CalibratedClassifierCV, calibration_curve
from sklearn.metrics import roc_auc_score, brier_score_loss, roc_curve
from sklearn.utils import resample
import warnings

# åŸºç¡€é…ç½®
warnings.filterwarnings('ignore')
optuna.logging.set_verbosity(optuna.logging.WARNING)

# è·¯å¾„ç®¡ç†
BASE_DIR = "../../"
INPUT_PATH = os.path.join(BASE_DIR, "data/cleaned/mimic_processed.csv")
JSON_FEAT_PATH = os.path.join(BASE_DIR, "artifacts/features/selected_features.json")
MODEL_ROOT = os.path.join(BASE_DIR, "artifacts/models")
RESULT_ROOT = os.path.join(BASE_DIR, "results/figures")
os.makedirs(os.path.join(BASE_DIR, "artifacts/scalers"), exist_ok=True)
os.makedirs(os.path.join(BASE_DIR, "results/tables"), exist_ok=True)
# =========================================================
# è¾…åŠ©å·¥å…·å‡½æ•°
# =========================================================
def get_auc_ci(model, X_test, y_test, n_bootstraps=1000):
    scores = []
    # ç¡®ä¿ X_test æ˜¯ array
    X_arr = np.array(X_test)
    y_arr = np.array(y_test)
    indices = np.arange(len(y_arr))
    
    for i in range(n_bootstraps):
        resample_idx = resample(indices, random_state=i)
        y_res = y_arr[resample_idx]
        if len(np.unique(y_res)) < 2: continue
        
        prob = model.predict_proba(X_arr[resample_idx])[:, 1]
        scores.append(roc_auc_score(y_res, prob))
    sorted_scores = np.sort(scores)
    if len(sorted_scores) == 0: return 0, 0
    return sorted_scores[int(0.025 * len(sorted_scores))], sorted_scores[int(0.975 * len(sorted_scores))]

# =========================================================
# æ ¸å¿ƒè®­ç»ƒæµæ°´çº¿
# =========================================================
def run_model_training_flow():
    # 1. åŠ è½½æ•°æ®ä¸ç‰¹å¾é…ç½®
    df = pd.read_csv(INPUT_PATH)
    with open(JSON_FEAT_PATH, 'r') as f:
        feature_config = json.load(f)
    
    global_performance = []

    for target in feature_config.keys():
        print(f"\n\n{'='*30} æ­£åœ¨åˆ†æç»“å±€: {target.upper()} {'='*30}")
        target_model_dir = os.path.join(MODEL_ROOT, target.lower())
        target_fig_dir = os.path.join(RESULT_ROOT, target.lower())
        for d in [target_model_dir, target_fig_dir]:
            os.makedirs(d, exist_ok=True)

        # 2. å‡†å¤‡è¯¥ç»“å±€ä¸“å±ç‰¹å¾é›†
        selected_features = feature_config[target]['features']
        X = df[selected_features].copy()
        y = df[target]
        subgroup = df['subgroup_no_renal']

        # 3. åˆ’åˆ†ä¸é¢„å¤„ç†
        X_train, X_test, y_train, y_test, _, sub_test = train_test_split(
            X, y, subgroup, test_size=0.2, random_state=42, stratify=y
        )
        imputer_pre = IterativeImputer(max_iter=10, random_state=42)
        scaler_pre = StandardScaler()
        X_train_pre = scaler_pre.fit_transform(imputer_pre.fit_transform(X_train))

        # 4. XGBoost Optuna è´å¶æ–¯å¯»ä¼˜
        print(f"ğŸ”¬ å¯åŠ¨ {target} çš„ XGBoost å¯»ä¼˜...")
        def objective(trial):
            param = {
                'n_estimators': trial.suggest_int('n_estimators', 100, 500),
                'max_depth': trial.suggest_int('max_depth', 3, 6),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),
                'subsample': trial.suggest_float('subsample', 0.6, 0.9),
                'random_state': 42, 'eval_metric': 'logloss'
            }
            model = XGBClassifier(**param)
            cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
            return cross_val_score(model, X_train_pre, y_train, cv=cv, scoring='roc_auc').mean()

        study = optuna.create_study(direction='maximize')
        study.optimize(objective, n_trials=30) 
        best_xgb = XGBClassifier(**study.best_params, random_state=42)

        # 5. æ¨¡å‹ç«èµ› (å«æ¦‚ç‡æ ¡å‡†)
        models = {
            "Logistic Regression": LogisticRegression(class_weight='balanced', max_iter=1000),
            "Random Forest": RandomForestClassifier(n_estimators=200, max_depth=5, random_state=42, class_weight='balanced'),
            "XGBoost": best_xgb,
            "SVM": SVC(probability=True, class_weight='balanced'),
            "Decision Tree": DecisionTreeClassifier(max_depth=5, class_weight='balanced')
        }

        calibrated_results = {}
        target_summary = []
        all_ci_stats = {}  # ä¿®æ­£ï¼šåˆå§‹åŒ–åœ¨ç»“å±€å¾ªç¯å†…ï¼Œæ¨¡å‹å¾ªç¯å¤–
        
        print("\n" + "-"*95)
        print(f"{'Algorithm':<20} | {'Main AUC (95% CI)':<30} | {'Brier':<10}")
        print("-" * 95)

        # é¢„å¤„ç†æµ‹è¯•é›†ï¼ˆä»…ä¸€æ¬¡ï¼‰
        X_test_pre = scaler_pre.transform(imputer_pre.transform(X_test))

        for name, m in models.items():
            # æ¦‚ç‡æ ¡å‡†
            clf = CalibratedClassifierCV(m, cv=3, method='isotonic', n_jobs=1)
            clf.fit(X_train_pre, y_train) 
            calibrated_results[name] = clf
            
            y_prob = clf.predict_proba(X_test_pre)[:, 1]

            # --- å…¨äººç¾¤è¯„ä¼° (Main) ---
            auc_main = roc_auc_score(y_test, y_prob)
            brier = brier_score_loss(y_test, y_prob)
            low_m, high_m = get_auc_ci(clf, X_test_pre, y_test)
            
            # --- äºšç»„è¯„ä¼° (No-Renal) ---
            sub_mask = (sub_test == 1).values
            if len(np.unique(y_test[sub_mask])) > 1:
                auc_sub = roc_auc_score(y_test[sub_mask], y_prob[sub_mask])
                low_s, high_s = get_auc_ci(clf, X_test_pre[sub_mask], y_test[sub_mask])
            else:
                auc_sub, low_s, high_s = 0, 0, 0

            # æ”¶é›†æ‰“å°ä¿¡æ¯ä¸æŠ¥è¡¨æ•°æ®
            main_auc_str = f"{auc_main:.3f} ({low_m:.3f}-{high_m:.3f})"
            sub_auc_str = f"{auc_sub:.3f} ({low_s:.3f}-{high_s:.3f})"
            print(f"{name:<20} | {main_auc_str:<30} | {brier:.4f}")
            
            target_summary.append({
                "Outcome": target,
                "Algorithm": name,
                "Main AUC": round(auc_main, 4),
                "Main AUC CI": main_auc_str,
                "No-Renal AUC CI": sub_auc_str,
                "Brier": round(brier, 4)
            })

            # æ”¶é›† CI æ•°æ®
            all_ci_stats[name] = {
                "main": [float(low_m), float(high_m)],
                "sub": [float(low_s), float(high_s)]
            } 

        # =========================================================
        # 6. ä¿å­˜èµ„äº§ (åœ¨æ¨¡å‹å¾ªç¯å¤–ç»Ÿä¸€ä¿å­˜è¯¥ç»“å±€æ‰€æœ‰èµ„äº§)
        # =========================================================
        print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜èµ„äº§è‡³: {target_model_dir}")
        
        joblib.dump(calibrated_results, os.path.join(target_model_dir, "all_models_dict.pkl"))
        joblib.dump(scaler_pre, os.path.join(target_model_dir, "scaler.pkl"))
        joblib.dump(imputer_pre, os.path.join(target_model_dir, "imputer.pkl"))
        joblib.dump(all_ci_stats, os.path.join(target_model_dir, "bootstrap_ci_stats.pkl"))
        
        # ä¿å­˜å¤–éƒ¨éªŒè¯åŒ…
        bundle = {'feature_order': X_train.columns.tolist(), 'target_outcome': target}
        joblib.dump(bundle, os.path.join(BASE_DIR, "artifacts/scalers/train_assets_bundle.pkl"))

        # æå–å¤šæ¨¡å‹ç‰¹å¾é‡è¦æ€§
        importance_list = []
        for name in ["XGBoost", "Random Forest", "Logistic Regression"]:
            if name in calibrated_results:
                raw_m = calibrated_results[name].base_estimator
                weights = raw_m.coef_.flatten() if name == "Logistic Regression" else raw_m.feature_importances_
                importance_list.append(pd.DataFrame({
                    'feature': selected_features,
                    'importance': weights,
                    'algorithm': name
                }))
        if importance_list:
            pd.concat(importance_list).to_csv(os.path.join(target_model_dir, "feature_importance.csv"), index=False)

        # ä¿å­˜è¯„ä¼°å¿«ç…§
        eval_assets = {
            'X_test_pre': X_test_pre, 'y_test': y_test.values, 
            'sub_mask': sub_mask, 'feature_names': selected_features
        }
        joblib.dump(eval_assets, os.path.join(target_model_dir, "eval_data.pkl"))

        # ç»˜å›¾ä¸æ±‡æ€»
        plot_performance(calibrated_results, X_test_pre, y_test, target, target_fig_dir)
        global_performance.extend(target_summary)

    # --- æœ€ç»ˆæ±‡æ€» ---
    perf_df = pd.DataFrame(global_performance)
    perf_df.to_csv(os.path.join(MODEL_ROOT, "performance_report.csv"), index=False)
    
    subgroup_table = perf_df.sort_values("Main AUC", ascending=False).drop_duplicates("Outcome")
    subgroup_table.to_csv(os.path.join(BASE_DIR, "results/tables/Table_Subgroup_Analysis.csv"), index=False)
    
    print(f"\nğŸš€ è®­ç»ƒæµç¨‹å…¨éƒ¨å®Œæˆï¼")

def plot_performance(models, X_test, y_test, target, save_path):
    """å°† ROC å’Œ Calibration æ›²çº¿ç”Ÿæˆä¸ºä¸¤ä¸ªç‹¬ç«‹çš„å­¦æœ¯å›¾ç‰‡"""
    
    # --- 1. ç»˜åˆ¶å¹¶ä¿å­˜ç‹¬ç«‹ ROC æ›²çº¿ ---
    plt.figure(figsize=(8, 7), dpi=300) # é«˜æ¸…åˆ†è¾¨ç‡
    for name, clf in models.items():
        y_prob = clf.predict_proba(X_test)[:, 1]
        fpr, tpr, _ = roc_curve(y_test, y_prob)
        auc_val = roc_auc_score(y_test, y_prob)
        plt.plot(fpr, tpr, lw=2, label=f"{name} (AUC = {auc_val:.3f})")
    
    plt.plot([0, 1], [0, 1], color='gray', linestyle='--', lw=1.5, alpha=0.7)
    plt.xlim([-0.02, 1.02])
    plt.ylim([-0.02, 1.02])
    plt.xlabel("False Positive Rate (1 - Specificity)", fontsize=12)
    plt.ylabel("True Positive Rate (Sensitivity)", fontsize=12)
    plt.title(f"ROC Curves: Predicted {target.upper()}", fontsize=14, fontweight='bold')
    plt.legend(loc="lower right", fontsize=10)
    plt.grid(alpha=0.3)
    plt.tight_layout()
    
    roc_path = os.path.join(save_path, "ROC_Curve.png")
    plt.savefig(roc_path, bbox_inches='tight')
    plt.close()

    # --- 2. ç»˜åˆ¶å¹¶ä¿å­˜ç‹¬ç«‹æ ¡å‡†æ›²çº¿ (Calibration) ---
    plt.figure(figsize=(8, 7), dpi=300)
    for name, clf in models.items():
        y_prob = clf.predict_proba(X_test)[:, 1]
        prob_true, prob_pred = calibration_curve(y_test, y_prob, n_bins=10)
        plt.plot(prob_pred, prob_true, marker='o', lw=2, ms=6, label=name)
    
    plt.plot([0, 1], [0, 1], color='gray', linestyle='--', lw=1.5, label='Perfectly Calibrated')
    plt.xlabel("Predicted Probability", fontsize=12)
    plt.ylabel("Actual Probability", fontsize=12)
    plt.title(f"Calibration Curves: Predicted {target.upper()}", fontsize=14, fontweight='bold')
    plt.legend(loc="upper left", fontsize=10)
    plt.grid(alpha=0.3)
    plt.tight_layout()
    
    calib_path = os.path.join(save_path, "Calibration_Curve.png")
    plt.savefig(calib_path, bbox_inches='tight')
    plt.close()
    
    print(f"âœ… å›¾ç‰‡å·²ä¿å­˜:\n   - ROC: {roc_path}\n   - Calib: {calib_path}")

if __name__ == "__main__":
    run_model_training_flow()
