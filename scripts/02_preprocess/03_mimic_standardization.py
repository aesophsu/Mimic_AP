import os
import joblib
import numpy as np
import pandas as pd
from tableone import TableOne
from sklearn.preprocessing import StandardScaler
from sklearn.experimental import enable_iterative_imputer  # å¿…é¡»å¯¼å…¥ä»¥å¯ç”¨ MICE
from sklearn.impute import IterativeImputer

# =========================================================
# 1. é…ç½®ä¸è·¯å¾„æ˜ å°„ (åŸºäº v3.0 ç›®å½•æ ‘)
# =========================================================
BASE_DIR = "../../"
INPUT_PATH = os.path.join(BASE_DIR, "data/cleaned/mimic_raw_scale.csv")
SAVE_DIR = os.path.join(BASE_DIR, "data/cleaned")

# èµ„äº§æŒä¹…åŒ–è·¯å¾„
ARTIFACT_DIR = os.path.join(BASE_DIR, "artifacts/scalers")
SCALER_PATH = os.path.join(ARTIFACT_DIR, "mimic_scaler.joblib")
IMPUTER_PATH = os.path.join(ARTIFACT_DIR, "mimic_mice_imputer.joblib")
SKEW_CONFIG_PATH = os.path.join(ARTIFACT_DIR, "skewed_cols_config.pkl")

REPORT_DIR = os.path.join(BASE_DIR, "results/tables")

os.makedirs(SAVE_DIR, exist_ok=True)
os.makedirs(ARTIFACT_DIR, exist_ok=True)
os.makedirs(REPORT_DIR, exist_ok=True)

def run_mimic_standardization():
    print("="*70)
    print("ğŸš€ å¯åŠ¨æ¨¡å— 03: äºšç»„åˆ’åˆ†ã€Log è½¬æ¢ã€MICE æ’è¡¥ä¸æ ‡å‡†åŒ–")
    print("="*70)
    
    if not os.path.exists(INPUT_PATH):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶ {INPUT_PATH}")
        return
    
    df = pd.read_csv(INPUT_PATH)

    # =========================================================
    # 2. äºšç»„åˆ’åˆ† (Subgroup Definition) ä¿æŒä¸å˜
    # =========================================================
    if 'creatinine_max' in df.columns and 'chronic_kidney_disease' in df.columns:
        df['subgroup_no_renal'] = (
            (df['creatinine_max'] < 1.5) & (df['chronic_kidney_disease'] == 0)
        ).astype(int)
        print(f"âœ… äºšç»„æ ‡è®°å®Œæˆ: 'æ— é¢„å­˜è‚¾æŸä¼¤' n = {df['subgroup_no_renal'].sum()}")
        
    # LASSO ä¸æ¥å—å­—ç¬¦ä¸²ï¼Œå¿…é¡»åœ¨æ­¤å¤„è½¬æ¢
    if 'gender' in df.columns:
        # å®šä¹‰å…¨é¢çš„æ˜ å°„å­—å…¸
        gender_map = {
            'M': 1, 'F': 0, 
            'Male': 1, 'Female': 0, 
            'MALE': 1, 'FEMALE': 0,
            1: 1, 0: 0, 1.0: 1, 0.0: 0
        }
        df['gender'] = df['gender'].map(gender_map)
        # å¡«å……ç¼ºå¤±æ€§åˆ«ï¼ˆå¯é€‰ï¼Œé€šå¸¸å»ºè®®ä¸­ä½æ•°æˆ–åˆ æ‰ï¼‰
        df['gender'] = df['gender'].fillna(df['gender'].mode()[0]).astype(int)
        print("âœ… å­—æ®µ 'gender' å·²å®Œæˆå½’ä¸€åŒ–æ˜ å°„ (1:Male, 0:Female)")

    # =========================================================
    # 3. ğŸ“Š è‡ªåŠ¨åŒ–ç»Ÿè®¡åˆ†æ (Table 1 & 2) - åŸºäºç‰©ç†å°ºåº¦
    # =========================================================
    clinical_features = [
        'admission_age', 'bmi', 'heart_failure', 'chronic_kidney_disease', 
        'malignant_tumor', 'bun_min', 'creatinine_max', 'lactate_max', 
        'pao2fio2ratio_min', 'wbc_max', 'alt_max', 'ast_max'
    ]
    outcome_cols = ['pof', 'mortality_28d', 'composite_outcome']
    cols_for_table = [c for c in (clinical_features + outcome_cols) if c in df.columns]
    categorical = [c for c in ['heart_failure', 'chronic_kidney_disease', 'malignant_tumor', 
                               'mortality_28d', 'composite_outcome', 'subgroup_no_renal'] if c in cols_for_table]
    nonnormal = [c for c in cols_for_table if c not in categorical]

    print("\nğŸ“Š æ­£åœ¨ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š (ç‰©ç†å°ºåº¦)...")
    t1 = TableOne(df, columns=cols_for_table, categorical=categorical, nonnormal=nonnormal, groupby='pof', pval=True)
    t1.to_csv(os.path.join(REPORT_DIR, "table1_baseline.csv"))
    
    if 'subgroup_no_renal' in df.columns:
        t2 = TableOne(df, columns=cols_for_table, categorical=categorical, nonnormal=nonnormal, groupby='subgroup_no_renal', pval=True)
        t2.to_csv(os.path.join(REPORT_DIR, "table2_renal_subgroup.csv"))
    print(f"âœ… Table 1 & 2 å·²å­˜è‡³: {REPORT_DIR}")

    # =========================================================
    # 4. æ³„éœ²é˜²æŠ¤ä¸ç‰¹å¾å‡†å¤‡ (ä¿®æ­£ç‰ˆ)
    # =========================================================
    # A. å®šä¹‰ä¸å‚ä¸å»ºæ¨¡çš„ ID ä¸æ—¶é—´åˆ—
    drop_from_modeling = [
        'subject_id', 'hadm_id', 'stay_id', 'database', 
        'admittime', 'dischtime', 'intime', 'deathtime', 'dod',
        'early_death_24_48h', 'hosp_mortality'
    ]
    
    # B. å®šä¹‰å¿…é¡»ä¿æŒåŸå§‹æ ¼å¼çš„åˆ— (æ ‡ç­¾ã€å­ç»“å±€ã€äºšç»„æ ‡è®°)
    protected_cols = [
        'pof', 'resp_pof', 'cv_pof', 'renal_pof', 
        'mortality_28d', 'composite_outcome', 'subgroup_no_renal',
        'gender', 'heart_failure', 'chronic_kidney_disease', 
        'malignant_tumor', 'mechanical_vent_flag', 'vaso_flag'
    ]
    
    df_model = df.drop(columns=[c for c in drop_from_modeling if c in df.columns])

    # å¼ºåˆ¶å°†ä¿æŠ¤åˆ—è½¬æ¢ä¸ºæ•´æ•° (é˜²æ­¢æ ‡å‡†åŒ–æ±¡æŸ“)
    for col in protected_cols:
        if col in df_model.columns:
            df_model[col] = df_model[col].fillna(0).astype(int)

    # å¼ºåˆ¶å‰”é™¤éæ•°å€¼åˆ— (å¦‚ Race ç­‰æ–‡æœ¬)
    remaining_text = df_model.select_dtypes(include=['object']).columns.tolist()
    if remaining_text:
        print(f"âš ï¸ è­¦å‘Š: å¼ºåˆ¶å‰”é™¤éæ•°å€¼åˆ—ä»¥é˜²æŠ¥é”™: {remaining_text}")
        df_model = df_model.drop(columns=remaining_text)
        
    # C. ç¡®å®šçœŸæ­£éœ€è¦â€œæ•°å€¼å¤„ç†â€çš„ç‰¹å¾ (æ’é™¤ä¿æŠ¤åˆ—)
    numeric_features = [c for c in df_model.select_dtypes(include=[np.number]).columns 
                        if c not in protected_cols]
    
    print(f"âœ… ç‰¹å¾åˆ†ç±»å®Œæˆ: æ•°å€¼ç‰¹å¾ {len(numeric_features)} ä¸ª, ä¿æŠ¤åˆ— {len(protected_cols)} ä¸ª")

    # =========================================================
    # 5. ğŸ§ª æ ¸å¿ƒå¢å¼ºï¼šåŠ¨æ€ Log1p è½¬æ¢ (å¤„ç†åæ€)
    # =========================================================
    skewed_cols = ['creatinine_max', 'creatinine_min', 'bun_max', 'bun_min',
                   'wbc_max', 'wbc_min', 'glucose_max', 'glucose_min',
                   'lactate_max', 'alt_max', 'ast_max', 'bilirubin_total_max']
    existing_skewed = [c for c in skewed_cols if c in numeric_features]
    
    print(f"ğŸ”„ æ‰§è¡Œ Log1p è½¬æ¢ (å¤„ç† {len(existing_skewed)} ä¸ªåæ€æŒ‡æ ‡)...")
    for col in existing_skewed:
        df_model[col] = np.log1p(df_model[col].clip(lower=0))
    
    joblib.dump(existing_skewed, SKEW_CONFIG_PATH)

    # =========================================================
    # 6. ğŸ§ª æ ¸å¿ƒå¢å¼ºï¼šMICE å¤šé‡æ’è¡¥ (ä»…é’ˆå¯¹æ•°å€¼ç‰¹å¾)
    # =========================================================
    print("ğŸ§ª å¯åŠ¨ MICE å¤šé‡æ’è¡¥ (ä»…å¤„ç† numeric_features)...")
    imputer = IterativeImputer(max_iter=10, random_state=42, initial_strategy='median')
    
    # æ³¨æ„ï¼šåªå¯¹æ•°å€¼åˆ—è¿›è¡Œ fit å’Œ transform
    df_model[numeric_features] = imputer.fit_transform(df_model[numeric_features])
    joblib.dump(imputer, IMPUTER_PATH)

    # =========================================================
    # 7. âš–ï¸ Z-score æ ‡å‡†åŒ– (ä»…é’ˆå¯¹æ•°å€¼ç‰¹å¾)
    # =========================================================
    # =========================================================
    # 7. âš–ï¸ Z-score æ ‡å‡†åŒ–
    # =========================================================
    print("âš–ï¸ æ‰§è¡Œ Z-score æ ‡å‡†åŒ–...")
    scaler = StandardScaler()
    
    # ä¿®å¤ç‚¹ï¼šå¼ºåˆ¶è½¬æ¢ä¸º DataFrame ä»¥ä¿æŒç‰¹å¾åï¼Œè™½ç„¶ StandardScaler æœ¬èº«ä¸å­˜ï¼Œ
    # ä½†æˆ‘ä»¬è¦åœ¨ bundle ä¸­æ‰‹åŠ¨å»ºç«‹åˆ—åæ˜ å°„ã€‚
    df_model[numeric_features] = scaler.fit_transform(df_model[numeric_features])
    joblib.dump(scaler, SCALER_PATH)

    # =========================================================
    # 8. ğŸ“¦ ã€å…³é”®æ–°å¢ã€‘: ç”Ÿæˆå¹¶ä¿å­˜è®­ç»ƒèµ„äº§æŸ (Artifact Bundle)
    # =========================================================
    print("\nğŸ“¦ æ­£åœ¨æ„å»ºè®­ç»ƒèµ„äº§æŸ (ç”¨äºè·¨åº“å¯¹é½)...")
    
    # è®¡ç®—ç‰©ç†å°ºåº¦ä¸‹çš„ä¸­ä½æ•°ï¼ˆåœ¨ df ä¸Šè®¡ç®—ï¼Œè€Œä¸æ˜¯ df_modelï¼‰
    # è¿™æ˜¯ä¸ºäº†ç»™ eICU æä¾›çœŸå®çš„ç‰©ç†å‚è€ƒ
    mimic_medians = df[numeric_features].median().to_dict()
    
    # æ„å»ºèµ„äº§å­—å…¸
    train_assets = {
        'skewed_cols': existing_skewed,      # å“ªäº›åˆ—åšäº† Log1p
        'medians': mimic_medians,            # ç‰©ç†ä¸­ä½æ•° (çº é”™å…³é”®)
        'feature_order': numeric_features,   # è®­ç»ƒæ—¶çš„ç‰¹å¾ç»å¯¹é¡ºåº
        'n_samples': len(df)
    }
    
    BUNDLE_PATH = os.path.join(ARTIFACT_DIR, "train_assets_bundle.pkl")
    joblib.dump(train_assets, BUNDLE_PATH)
    
    # --- DEBUG å¢å¼ºè¾“å‡º ---
    print("-" * 30)
    print(f"âœ… èµ„äº§æŸå·²æŒä¹…åŒ–: {BUNDLE_PATH}")
    print(f"ğŸ“Š æŠ½æ ·æ ¸æŸ¥ (MIMIC ç‰©ç†ä¸­ä½æ•°):")
    for check_f in ['admission_age', 'creatinine_max', 'ph_min']:
        if check_f in mimic_medians:
            print(f"   - {check_f:<15}: {mimic_medians[check_f]:.4f}")
    print("-" * 30)

    # =========================================================
    # 9. æ£€æŸ¥å¹¶ä¿å­˜
    # =========================================================
    # ... åŸæœ‰çš„ä¿å­˜ä»£ç  ...
    processed_path = os.path.join(SAVE_DIR, "mimic_processed.csv")
    df_model.to_csv(processed_path, index=False)   
    
    print(f"âœ… æ¨¡å— 03 å¤„ç†å®Œæˆï¼å»ºæ¨¡å¼ é‡ç»´åº¦: {df_model.shape}")
    print("-" * 70)

if __name__ == "__main__":
    run_mimic_standardization()
