import os
import json
import joblib
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    confusion_matrix, f1_score, roc_auc_score, 
    average_precision_score, brier_score_loss, roc_curve
)

# ===================== é…ç½®è·¯å¾„ =====================
BASE_DIR = "../.."
MODEL_ROOT = os.path.join(BASE_DIR, "artifacts/models")
EICU_DIR = os.path.join(BASE_DIR, "data/external")
TABLE_DIR = os.path.join(BASE_DIR, "results/tables")
FIGURE_DIR = os.path.join(BASE_DIR, "results/figures/comparison")

# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
for d in [TABLE_DIR, FIGURE_DIR]:
    os.makedirs(d, exist_ok=True)

TARGETS = ['pof', 'mortality', 'composite']

def load_external_validation_assets(target):
    """
    åŸºäºç¬¬ 06 æ­¥ä¿å­˜çš„èµ„äº§ï¼ŒåŠ è½½æ¨¡å‹ã€æ ‡å‡†åŒ–å™¨å’Œç‰¹å¾æ¸…å•
    """
    target_dir = os.path.join(MODEL_ROOT, target.lower())
    
    # 1. åŠ è½½æ¨¡å‹å­—å…¸
    models_path = os.path.join(target_dir, "all_models_dict.pkl")
    if not os.path.exists(models_path):
        raise FileNotFoundError(f"æœªæ‰¾åˆ°æ¨¡å‹å­—å…¸: {models_path}")
    models = joblib.load(models_path)
    
    # 2. åŠ è½½éƒ¨ç½²åŒ…
    bundle_path = os.path.join(target_dir, "deploy_bundle.pkl")
    if not os.path.exists(bundle_path):
        raise FileNotFoundError(f"æœªæ‰¾åˆ°éƒ¨ç½²åŒ…: {bundle_path}")
    bundle = joblib.load(bundle_path)
    
    # 3. åŠ è½½é˜ˆå€¼å­—å…¸ (æ³¨æ„ï¼šè¿™é‡Œä¸å†è°ƒç”¨ .getï¼Œè€Œæ˜¯ç›´æ¥åŠ è½½æ•´ä¸ª JSON å¯¹è±¡)
    thresh_path = os.path.join(target_dir, "thresholds.json")
    threshold_data = {} # é»˜è®¤ä¸ºç©ºå­—å…¸
    if os.path.exists(thresh_path):
        with open(thresh_path, 'r') as f:
            threshold_data = json.load(f)
    
    return models, bundle['feature_names'], bundle['scaler'], threshold_data

def process_and_align_eicu(target, features, scaler):
    """
    è¯»å– eICU æ•°æ®å¹¶åº”ç”¨ MIMIC çš„æ ‡å‡†åŒ–å‚æ•°è¿›è¡Œå¯¹é½
    """
    eicu_path = os.path.join(EICU_DIR, f"eicu_processed_{target}.csv")
    if not os.path.exists(eicu_path):
        raise FileNotFoundError(f"æœªæ‰¾åˆ° eICU æ•°æ®: {eicu_path}")
    
    df = pd.read_csv(eicu_path)
    # å¼ºåˆ¶è½¬ä¸ºå­—ç¬¦ä¸²åˆ—å
    df.columns = [str(c) for c in df.columns]
    
    # æŒ‰ç…§ MIMIC çš„ç‰¹å¾é¡ºåºæå–ï¼Œç¼ºå¤±åˆ—è¡¥ 0
    X_list = []
    missing_count = 0
    for f in features:
        f_str = str(f)
        if f_str in df.columns:
            X_list.append(df[f_str])
        else:
            X_list.append(pd.Series(np.zeros(len(df)), name=f_str))
            missing_count += 1
            
    if missing_count > 0:
        print(f"    [æç¤º] eICU ç¼ºå¤± {missing_count} ä¸ªç‰¹å¾ï¼Œå·²è‡ªåŠ¨è¡¥ 0")
        
    X_raw = pd.concat(X_list, axis=1)
    y_true = df[target].values
    
    # åº”ç”¨ç¬¬ 06 æ­¥çš„æ ‡å‡†åŒ–å™¨ (æ³¨æ„ï¼šæ­¤å¤„åª transformï¼Œä¸ fit)
    X_scaled = scaler.transform(X_raw)
    
    return X_scaled, y_true

def compute_metrics_ci(y_true, y_prob, n_bootstraps=1000, seed=42):
    """åŒæ­¥è®¡ç®— AUC, AUPRC, Brier çš„ 95% CI"""
    rng = np.random.RandomState(seed)
    scores = {'auc': [], 'auprc': [], 'brier': []}
    
    for i in range(n_bootstraps):
        idx = rng.randint(0, len(y_true), len(y_true))
        if len(np.unique(y_true[idx])) < 2: continue
        scores['auc'].append(roc_auc_score(y_true[idx], y_prob[idx]))
        scores['auprc'].append(average_precision_score(y_true[idx], y_prob[idx]))
        scores['brier'].append(brier_score_loss(y_true[idx], y_prob[idx]))
    
    results = {}
    for k, v in scores.items():
        sorted_v = np.sort(v)
        results[k] = (sorted_v[int(0.025 * len(v))], sorted_v[int(0.975 * len(v))])
    return results

def plot_roc_comparison(target, mimic_tuple, eicu_tuple):
    """
    åŒ»å­¦çº§ ROC å¯¹æ¯”å›¾ï¼šå±•ç¤º MIMIC-IV å†…éƒ¨éªŒè¯ä¸ eICU å¤–éƒ¨éªŒè¯çš„è¿ç§»è¡¨ç°
    mimic_tuple: (auc, fpr, tpr, ci_tuple)
    eicu_tuple: (auc, fpr, tpr, ci_tuple)
    """
    m_auc, m_fpr, m_tpr, m_ci = mimic_tuple
    e_auc, e_fpr, e_tpr, e_ci = eicu_tuple

    plt.figure(figsize=(6, 6), dpi=300)
    plt.rcParams['font.sans-serif'] = ['Arial']
    
    # ç»˜åˆ¶æ— åŒºåˆ†èƒ½åŠ›çº¿ï¼ˆå¯¹è§’çº¿ï¼‰
    plt.plot([0, 1], [0, 1], color='#bdc3c7', linestyle='--', lw=1.2, alpha=0.8)
    
    # 1. ç»˜åˆ¶ MIMIC å†…éƒ¨éªŒè¯æ›²çº¿ (è“è‰²è™šçº¿)
    m_label = f'MIMIC-IV Internal (AUC: {m_auc:.3f} [{m_ci[0]:.3f}-{m_ci[1]:.3f}])'
    plt.plot(m_fpr, m_tpr, linestyle='--', color='#3498db', lw=2, label=m_label)
    
    # 2. ç»˜åˆ¶ eICU å¤–éƒ¨éªŒè¯æ›²çº¿ (æ·±é»‘è‰²å®çº¿)
    e_label = f"eICU External (AUC: {e_auc:.3f} [{e_ci[0]:.3f}-{e_ci[1]:.3f}])"
    plt.plot(e_fpr, e_tpr, color='#2c3e50', lw=2.5, label=e_label)
    
    # ç»†èŠ‚ç¾åŒ–
    plt.xlabel("False Positive Rate (1 - Specificity)", fontsize=11, labelpad=8)
    plt.ylabel("True Positive Rate (Sensitivity)", fontsize=11, labelpad=8)
    plt.title(f"Model Generalization: {target.upper()}", fontsize=13, fontweight='bold', pad=15)
    
    # å›¾ä¾‹ï¼šç½®äºå³ä¸‹è§’ï¼Œå–æ¶ˆè¾¹æ¡†
    plt.legend(loc="lower right", frameon=False, fontsize=8.5)
    
    # åæ ‡è½´ä¸æ ·å¼ç¾åŒ–
    ax = plt.gca()
    for spine in ['top', 'right']: ax.spines[spine].set_visible(False)
    ax.set_xlim([-0.01, 1.01])
    ax.set_ylim([-0.01, 1.01])
    plt.grid(color='whitesmoke', linestyle='-', linewidth=0.5)
    plt.tight_layout()

    # å¯¼å‡º
    base_path = os.path.join(FIGURE_DIR, f"ROC_External_{target}")
    plt.savefig(f"{base_path}.pdf", bbox_inches='tight')
    plt.savefig(f"{base_path}.png", bbox_inches='tight', dpi=600)
    plt.close()

def run_single_validation(target, mimic_auc_ref):
    """
    æ‰§è¡Œå•ä¸ªç»“å±€ç›®æ ‡çš„ 5 ç§æ¨¡å‹éªŒè¯
    é€‚é…å¤šæ¨¡å‹ä¸“å±é˜ˆå€¼å­—å…¸ï¼Œå¹¶å¯¹æ¯” MIMIC çœŸå®åŸºå‡†
    """
    print(f"\n>>> æ­£åœ¨åˆ†æç»“å±€: {target.upper()}")
    results = []
    
    try:
        # 1. åŠ è½½èµ„äº§ (æ­¤æ—¶ threshold_dict åŒ…å«å„æ¨¡å‹ä¸“å±é˜ˆå€¼)
        models, features, scaler, threshold_dict = load_external_validation_assets(target)
        X_eicu, y_eicu = process_and_align_eicu(target, features, scaler)
        y_eicu = np.array(y_eicu).astype(int) # ç¡®ä¿æ ‡ç­¾ä¸ºæ•´å‹
        
        # 1.1 åŠ è½½ MIMIC å†…éƒ¨éªŒè¯çœŸå®æ•°æ®ç”¨äºç»˜å›¾åŸºå‡†
        eval_path = os.path.join(MODEL_ROOT, target.lower(), "eval_data.pkl")
        eval_data = joblib.load(eval_path)
        y_prob_mimic = models["XGBoost"].predict_proba(eval_data['X_test_pre'])[:, 1]
        fpr_m, tpr_m, _ = roc_curve(eval_data['y_test'], y_prob_mimic)
        auc_m_real = roc_auc_score(eval_data['y_test'], y_prob_mimic)
        
        print(f"    -> eICU æ ·æœ¬é‡: {len(y_eicu)} | æ­£ä¾‹ç‡: {y_eicu.mean():.2%}")
        header = f"{'Algorithm':<20} | {'AUC (95% CI)':<22} | {'Brier':<8} | {'Sens':<8}"
        print(f"    {header}\n    {'-' * len(header)}")

        for name, model in models.items():
            # 2. åŠ¨æ€åŒ¹é…è¯¥æ¨¡å‹çš„æœ€ä½³é˜ˆå€¼
            current_thresh = threshold_dict.get(name, 0.5)
            
            # 3. æ¨¡å‹é¢„æµ‹ä¸æ€§èƒ½è¯„ä¼°
            y_prob = model.predict_proba(X_eicu)[:, 1]
            y_pred = (y_prob >= current_thresh).astype(int)
            
            # è®¡ç®—åŒ…å« 95% CI çš„å¤šç»´æŒ‡æ ‡
            cis = compute_metrics_ci(y_eicu, y_prob) 
            auc = roc_auc_score(y_eicu, y_prob)
            brier = brier_score_loss(y_eicu, y_prob)
            
            # è®¡ç®—æ•æ„Ÿåº¦ä¸ç‰¹å¼‚åº¦
            tn, fp, fn, tp = confusion_matrix(y_eicu, y_pred).ravel()
            sens = tp / (tp + fn) if (tp + fn) > 0 else 0
            spec = tn / (tn + fp) if (tn + fp) > 0 else 0

            # 4. æ§åˆ¶å°å®æ—¶è¾“å‡ºç»“æœ
            auc_display = f"{auc:.3f} ({cis['auc'][0]:.3f}-{cis['auc'][1]:.3f})"
            print(f"    {name:<20} | {auc_display:<22} | {brier:.4f} | {sens:.4f}")

            # 5. ç»“æœæ”¶é›†
            results.append({
                'Target': target, 'Algorithm': name, 
                'AUC': auc, 'AUC_Low': cis['auc'][0], 'AUC_High': cis['auc'][1],
                'Brier': brier, 'Sensitivity': sens, 'Specificity': spec,
                'AUPRC': average_precision_score(y_eicu, y_prob),
                'Threshold': current_thresh
            })

            # 6. ç»˜åˆ¶ä¸»æ¨¡å‹ (XGBoost) çš„è·¨ä¸­å¿ƒå¯¹æ¯”å›¾
            # ... ä¹‹å‰çš„ä»£ç  (åœ¨ XGBoost å¾ªç¯å†…) ...
            if name == "XGBoost":
                 fpr_e, tpr_e, _ = roc_curve(y_eicu, y_prob)
                 cis_m = compute_metrics_ci(eval_data['y_test'], y_prob_mimic)
                 plot_roc_comparison(
                    target, 
                    (auc_m_real, fpr_m, tpr_m, cis_m['auc']),
                    (auc, fpr_e, tpr_e, cis['auc'])
                )
        return results

    except Exception as e:
        print(f"    [å¤±è´¥] {target}: {str(e)}")
        return None

def plot_external_comparison_summary(csv_path):
    """åŒ»å­¦çº§æ€§èƒ½æ±‡æ€»å›¾ï¼šç®—æ³•æ¨ªå‘å¤§æ¯”æ‹¼ (ä¿®å¤ç‰ˆ)"""
    df = pd.read_csv(csv_path)
    sns.set_context("paper", font_scale=1.2)
    sns.set_style("ticks")
    
    # å®½è¡¨è½¬é•¿è¡¨
    plot_df = df.melt(id_vars=['Target', 'Algorithm'], 
                      value_vars=['AUC', 'Sensitivity', 'Specificity'],
                      var_name='Metric', value_name='Score')

    # ä¿®å¤ï¼šä½¿ç”¨ markersize ä»£æ›¿ sï¼Œä½¿ç”¨ linestyle='none' ä»£æ›¿ join=False
    g = sns.catplot(
        data=plot_df, x='Target', y='Score', hue='Algorithm',
        col='Metric', kind='point', 
        linestyle='none', 
        palette='Set1', 
        markers=['o', 's', 'D', 'X', 'P'],
        dodge=0.5, height=5, aspect=0.7,
        markersize=10  # æ­£ç¡®çš„ Line2D å¤§å°å‚æ•°
    )

    # å¸ƒå±€ä¸åæ ‡è½´è°ƒæ•´
    g.set_titles("{col_name}", size=14, fontweight='bold')
    g.set_axis_labels("", "Metric Score", size=12)
    g.set(ylim=(0, 1.05))
    
    for ax in g.axes.flat:
        # æ·»åŠ  0.8 å’Œ 0.9 åŸºå‡†çº¿
        ax.axhline(0.8, color='#bdc3c7', linestyle='--', lw=0.8, alpha=0.5)
        ax.axhline(0.9, color='#bdc3c7', linestyle='--', lw=0.8, alpha=0.5)
        
        # ç¨³å¥çš„åˆ»åº¦æ ‡ç­¾å¤§å†™é€»è¾‘
        ticks = ax.get_xticks()
        ax.set_xticks(ticks)
        labels = [t.get_text().upper() for t in ax.get_xticklabels()]
        ax.set_xticklabels(labels)

    g.fig.subplots_adjust(top=0.88)
    g.fig.suptitle('External Validation Performance Across eICU Cohort', 
                   fontsize=16, fontweight='bold')

    base_path = os.path.join(FIGURE_DIR, "Table4_Performance_Visualization")
    g.savefig(f"{base_path}.pdf", bbox_inches='tight')
    g.savefig(f"{base_path}.png", bbox_inches='tight', dpi=600)
    print(f"ğŸ“Š å¤–éƒ¨éªŒè¯æ±‡æ€»å›¾å·²æˆåŠŸç”Ÿæˆ (PDF/PNG)")

def get_mimic_base_auc(target, algorithm="XGBoost"):
    """ä» 06 æ­¥ç”Ÿæˆçš„æ€§èƒ½æŠ¥å‘Šä¸­åŠ¨æ€æå– MIMIC å®é™… AUC"""
    report_path = os.path.join(MODEL_ROOT, "performance_report.csv")
    try:
        df_perf = pd.read_csv(report_path)
        # åŒ¹é…ç»“å±€å’ŒæŒ‡å®šç®—æ³•ï¼ˆé€šå¸¸ä»¥ XGBoost ä½œä¸ºå¯¹æ¯”åŸºå‡†ï¼‰
        match = df_perf[(df_perf['Outcome'] == target.lower()) & 
                        (df_perf['Algorithm'] == algorithm)]
        return match['Main AUC'].values[0] if not match.empty else 0.85
    except Exception:
        # å…œåº•é¢„è®¾å€¼
        return {'pof': 0.882, 'mortality': 0.845, 'composite': 0.867}.get(target.lower(), 0.85)

def main():
    print(f"{'='*40}\nå¯åŠ¨æ¨¡å— 11: eICU å¤–éƒ¨éªŒè¯ (åŠ¨æ€åŸºå‡†ç‰ˆ)\n{'='*40}")
    performance_table = []

    for target in TARGETS:
        # åŠ¨æ€è·å–è¯¥ç»“å±€åœ¨ MIMIC ä¸Šçš„å®é™…è¡¨ç°ä½œä¸ºç»˜å›¾å‚è€ƒçº¿
        mimic_auc_ref = get_mimic_base_auc(target)
        print(f"\n[åŸºå‡†ç¡®è®¤] {target.upper()} MIMIC å®é™… AUC: {mimic_auc_ref:.4f}")
        
        results = run_single_validation(target, mimic_auc_ref)
        if results:
            performance_table.extend(results)
            
    if performance_table:
        # æ•°æ®æ•´ç†ä¸ä¿å­˜
        df_final = pd.DataFrame(performance_table)
        df_final = df_final.sort_values(['Target', 'AUC'], ascending=[True, False])
        
        csv_path = os.path.join(TABLE_DIR, "Table4_External_Validation.csv")
        df_final.to_csv(csv_path, index=False)
        print(f"\nâœ… å¤–éƒ¨éªŒè¯ç»“æœå·²å¯¼å‡º: {csv_path}")

        # ç”ŸæˆåŒ»å­¦å‡ºç‰ˆçº§æ±‡æ€»å›¾
        try:
            plot_external_comparison_summary(csv_path)
        except Exception as e:
            print(f"âš ï¸ æ±‡æ€»å›¾ç”Ÿæˆå¤±è´¥: {e}")

    print("\n[å®Œæˆ] å¤–éƒ¨éªŒè¯æµå·²ç»“æŸã€‚ä¸‹ä¸€æ­¥: 12_model_interpretation_shap.py")

if __name__ == "__main__":
    main()
