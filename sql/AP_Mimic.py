import os
import joblib
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import shap
import seaborn as sns
from scipy.stats import norm

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.calibration import CalibratedClassifierCV, calibration_curve
from sklearn.metrics import roc_auc_score, roc_curve, brier_score_loss, confusion_matrix
from imblearn.over_sampling import SMOTE

# =========================================================
# 0. å…¨å±€é…ç½®ä¸è·¯å¾„
# =========================================================
RANDOM_STATE = 42
MAX_LASSO_FEATURES = 15
N_BOOTSTRAPS = 1000

BASE_DIR = ".."
DATA_PATH = os.path.join(BASE_DIR, "data/ap_final_analysis_cohort.csv")
SAVE_DIR_FIG = os.path.join(BASE_DIR, "figures/final_robust")
SAVE_DIR_MODEL = os.path.join(BASE_DIR, "models")
SAVE_DIR_DATA = os.path.join(BASE_DIR, "data/cleaned") # æ–°å¢æ¸…æ´—åæ•°æ®å­˜æ”¾è·¯å¾„

for d in [SAVE_DIR_FIG, SAVE_DIR_MODEL, SAVE_DIR_DATA]:
    os.makedirs(d, exist_ok=True)

# =========================================================
# 1. æ·±åº¦æ•°æ®æ¸…æ´—å‡½æ•° (åˆ†é˜¶æ®µå¤„ç†)
# =========================================================

def clinical_winsorization(data):
    """é˜¶æ®µ1ï¼šç¦»ç¾¤å€¼ç›–å¸½ (ç”¨äº Table 1)"""
    df = data.copy()
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        # æ’é™¤æ ‡ç­¾å’Œåˆ†ç±»æ ‡è¯†ç¬¦
        if col not in ['pof', 'gender_num', 'subject_id', 'hadm_id', 'stay_id'] and '_flag' not in col:
            upper = df[col].quantile(0.99)
            lower = df[col].quantile(0.01)
            df[col] = df[col].clip(lower, upper)
    return df
def clinical_feature_engineering(data):
    """
    é˜¶æ®µ2ï¼šå•ä½è‡ªåŠ¨æ ¡å‡†ã€éçº¿æ€§å˜æ¢ä¸å…±çº¿æ€§å¤„ç†
    """
    df = data.copy()
    
    # === 1. å•ä½è‡ªåŠ¨æ ¡å‡† (é’ˆå¯¹ Table 1 å‘ç°çš„å•ä½ä¸ç»Ÿä¸€é—®é¢˜) ===
    
    # A. çº¤ç»´è›‹ç™½åŸ (Fibrinogen) æ ¡å‡†
    # MIMIC é€šå¸¸ä½¿ç”¨ mg/dL (å‡å€¼çº¦ 300-400)ï¼ŒeICU å¯èƒ½ä½¿ç”¨ g/L (å‡å€¼çº¦ 2-4)
    # å¦‚æœä¸­ä½æ•°éå¸¸å°ï¼ˆæ¯”å¦‚ < 50ï¼‰ï¼Œåˆ™åˆ¤å®šä¸º g/Lï¼Œä¹˜ä»¥ 100 è½¬æ¢æˆ mg/dL
    if 'fibrinogen_max' in df.columns:
        median_val = df['fibrinogen_max'].median()
        if not pd.isna(median_val) and median_val < 50:
            print(f"ğŸ”„ æ£€æµ‹åˆ° Fibrinogen å•ä½å¼‚å¸¸ (Median={median_val:.2f}), æ­£åœ¨ä» g/L è½¬æ¢ä¸º mg/dL...")
            df['fibrinogen_max'] = df['fibrinogen_max'] * 100

    # B. é˜´ç¦»å­é—´éš™ (Anion Gap) æ ¡å‡†
    # å¦‚æœå‘ç° eICU çš„ Anion Gap æ•´ä½“åä½ï¼Œå¯èƒ½æ˜¯è®¡ç®—å…¬å¼å·®å¼‚ï¼Œ
    # ä¸´åºŠç ”ç©¶ä¸­é€šå¸¸ä½¿ç”¨ Z-Score æˆ–ç›´æ¥å¯¹é½å‡å€¼ï¼ˆæ­¤å¤„å»ºè®®å…ˆæ£€æŸ¥æ˜¯å¦ä¸ºé‡çº²é—®é¢˜ï¼‰
    # å¦‚æœæœ‰æ˜ç¡®çš„å€æ•°å…³ç³»ï¼Œåœ¨æ­¤å¤„æ·»åŠ è½¬æ¢é€»è¾‘
    
    # === 2. åæ€æŒ‡æ ‡ Log è½¬æ¢ (é’ˆå¯¹æ¨¡å‹ä¼˜åŒ–) ===
    # åŒ…å«ä½  SHAP å›¾ä¸­é‡è¦çš„å‡ ä¸ªè¿ç»­å˜é‡
    skewed_cols = [
        'amylase_max', 'lipase_max', 'crp_max', 'fibrinogen_max', 
        'wbc_max', 'creatinine_min', 'bun_max', 'glucose_max', 'lactate_max'
    ]
    for col in skewed_cols:
        if col in df.columns:
            # ä½¿ç”¨ log1p (ln(x+1)) å¤„ç†ï¼Œå¹¶è¿›è¡Œ clip é˜²æ­¢è´Ÿå€¼
            df[col] = np.log1p(df[col].astype(float).clip(lower=0))
    
    # === 3. å…±çº¿æ€§æ£€æµ‹ (ä¿æŒåŸæœ‰é€»è¾‘) ===
    numeric_df = df.select_dtypes(include=[np.number])
    corr_matrix = numeric_df.corr().abs()
    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
    to_drop = [column for column in upper.columns if any(upper[column] > 0.90)]
    # ç¡®ä¿ä¸åˆ æ‰ç›®æ ‡å˜é‡å’Œå…³é”® ID
    to_drop = [c for c in to_drop if c not in ['pof', 'subject_id', 'hadm_id', 'stay_id']]
    
    if to_drop:
        print(f"ğŸ“‰ ç§»é™¤é«˜åº¦å…±çº¿æ€§ç‰¹å¾ (>0.9): {to_drop}")
        df = df.drop(columns=to_drop)
        
    return df

# =========================================================
# 2. æ•°æ®å‡†å¤‡ä¸ä¿å­˜é€»è¾‘
# =========================================================
df_raw = pd.read_csv(DATA_PATH)

# --- ç”Ÿæˆå¹¶ä¿å­˜ Table 1 ç‰ˆæœ¬ ---
# ä»…æˆªæ–­å¼‚å¸¸å€¼ï¼Œä¿ç•™åŸå§‹é‡çº² (mg/dL, mmol/L ç­‰)
df_table1 = clinical_winsorization(df_raw)
df_table1.to_csv(os.path.join(SAVE_DIR_DATA, "mimic_for_table1.csv"), index=False)
print("âœ… Table 1 æ•°æ®å·²ä¿å­˜ (ä¿ç•™åŸå§‹å•ä½).")

# --- ç”Ÿæˆå¹¶ä¿å­˜æ¨¡å‹ç‰ˆæœ¬ ---
# æ‰§è¡Œ Log è½¬æ¢å’Œå…±çº¿æ€§å‰”é™¤
df_model_ready = clinical_feature_engineering(df_table1)
df_model_ready.to_csv(os.path.join(SAVE_DIR_DATA, "mimic_for_model.csv"), index=False)
print("âœ… æ¨¡å‹è®­ç»ƒæ•°æ®å·²ä¿å­˜ (å·²æ‰§è¡Œ Log è½¬æ¢).")

# =========================================================
# 3. å»ºæ¨¡æµç¨‹ (ä½¿ç”¨ df_model_ready)
# =========================================================
TARGET = "pof"

# A. ä¸´åºŠç»“å±€ä¸æ ‡è¯†ç¬¦ (å¿…é¡»æ’é™¤)
IDENTIFIERS = ["stay_id", "hadm_id", "subject_id", "intime", "admittime", 
               "dischtime", "deathtime", "dod", "race", "insurance"]

# B. æ•°æ®æ³„éœ²æŒ‡æ ‡ (ç»å¯¹ä¸èƒ½å‡ºç°åœ¨é¢„æµ‹å› å­ä¸­)
LEAKAGE_METRICS = [
    "los",                   # ä½é™¢æ—¶é•¿æ˜¯ç»“æœï¼Œä¸æ˜¯é¢„æµ‹å› å­
    "mortality_28d",         # æ­»äº¡ç»“å±€
    "resp_pof", "cv_pof", "renal_pof" # ç»“å±€çš„å­ç»„æˆéƒ¨åˆ†
]

# C. æ²»ç–—å¹²é¢„æŒ‡æ ‡ (å› æœå€’ç½®é£é™©)
# POF çš„å®šä¹‰ä¾èµ–äºè¿™äº›å¹²é¢„ï¼ŒåŒ…å«å®ƒä»¬ä¼šè®© AUC è™šé«˜
TREATMENT_INTERVENTION = [
    "vaso_flag",             # å‡å‹è¯ä½¿ç”¨æƒ…å†µ
    "mechanical_vent_flag",  # æœºæ¢°é€šæ°”ä½¿ç”¨æƒ…å†µ
]

# D. è¯„åˆ†ç³»ç»ŸåŠå…¶å¼ºç›¸å…³å­é¡¹
# å¦‚æœä½ çš„ç»“å±€ POF æ˜¯åŸºäº SOFA å®šä¹‰çš„ï¼Œæœ€å¥½æ’é™¤å…¶å¯¹åº”çš„ç”Ÿç†æ¯”å€¼
SCORING_SYSTEMS = [
    "sofa_score", "apsiii", "sapsii", "oasis", "lods",
    "pao2fio2ratio_min"      # æ°§åˆæŒ‡æ•°æ˜¯ SOFA å‘¼å¸è¯„åˆ†çš„æ ¸å¿ƒï¼Œå»ºè®®æ’é™¤
]

# æ±‡æ€»æœ€ç»ˆæ’é™¤åˆ—è¡¨
BASE_EXCLUDE = [TARGET] + IDENTIFIERS + LEAKAGE_METRICS + TREATMENT_INTERVENTION + SCORING_SYSTEMS

X = pd.get_dummies(df_model_ready.drop(columns=[c for c in BASE_EXCLUDE if c in df_model_ready.columns]), drop_first=True)
y = df_model_ready[TARGET]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=RANDOM_STATE)

# æ ‡å‡†åŒ– (Scaler ä»…åº”ç”¨äºæ¨¡å‹è¾“å…¥)
scaler_pipe = Pipeline([
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler())
])
X_train_s = pd.DataFrame(scaler_pipe.fit_transform(X_train), columns=X_train.columns)
X_test_s = pd.DataFrame(scaler_pipe.transform(X_test), columns=X_test.columns)

# --- LASSO ç­›é€‰ ---
lasso = LogisticRegressionCV(Cs=15, cv=5, penalty="l1", solver="liblinear", scoring="roc_auc", random_state=RANDOM_STATE)
lasso.fit(X_train_s, y_train)
selected_feats = pd.Series(lasso.coef_[0], index=X_train_s.columns).abs().sort_values(ascending=False).head(MAX_LASSO_FEATURES).index.tolist()

X_train_l, X_test_l = X_train_s[selected_feats], X_test_s[selected_feats]

# --- SMOTE ---
X_res, y_res = SMOTE(random_state=RANDOM_STATE).fit_resample(X_train_l, y_train)

# --- è®­ç»ƒä¸æ ¡å‡† ---
results_store = {}
models = {
    "XGBoost": XGBClassifier(n_estimators=300, max_depth=4, learning_rate=0.05, subsample=0.8, random_state=RANDOM_STATE),
    "Logistic": LogisticRegression(class_weight='balanced', random_state=RANDOM_STATE)
}

print(f"\n{'Model':<12} | {'AUC (95% CI)':<22} | {'Sens':<6} | {'Spec':<6} | {'Brier':<6}")
print("-" * 75)

for name, m in models.items():
    calibrated = CalibratedClassifierCV(m, method='isotonic', cv=3)
    calibrated.fit(X_res, y_res)
    y_prob = calibrated.predict_proba(X_test_l)[:, 1]
    
    auc = roc_auc_score(y_test, y_prob)
    fpr, tpr, thresholds = roc_curve(y_test, y_prob)
    ix = np.argmax(tpr - fpr)
    sens, spec = tpr[ix], 1 - fpr[ix]
    
    print(f"{name:<12} | {auc:.3f} | {sens:.3f} | {spec:.3f} | {brier_score_loss(y_test, y_prob):.3f}")
    results_store[name] = {"y_prob": y_prob, "model": calibrated}

# =========================================================
# 4. å¯è§†åŒ–
# =========================================================

# --- A. æ ¡å‡†æ›²çº¿ ---
plt.figure(figsize=(8, 6))
for name in results_store:
    prob_true, prob_pred = calibration_curve(y_test, results_store[name]["y_prob"], n_bins=10)
    plt.plot(prob_pred, prob_true, "s-", label=f"{name}")
plt.plot([0, 1], [0, 1], "k--", alpha=0.5)
plt.title("Calibration Curve (Isotonic)")
plt.xlabel("Predicted Probability")
plt.ylabel("Actual Probability")
plt.legend()
plt.savefig(os.path.join(SAVE_DIR_FIG, "Calibration_Curve.png"))

# --- B. SHAP è§£é‡Š (é’ˆå¯¹ XGBoost) ---
print("\n--- Generating SHAP Analysis ---")
try:
    # å…¼å®¹æ€§ä¿®æ­£ï¼šå°è¯•ä½¿ç”¨ .estimatorï¼Œå¦‚æœä¸è¡Œåˆ™ fallback åˆ° .base_estimator
    calibrated_model = results_store["XGBoost"]["model"]
    first_clf = calibrated_model.calibrated_classifiers_[0]
    
    if hasattr(first_clf, 'estimator'):
        best_xgb = first_clf.estimator
    else:
        best_xgb = first_clf.base_estimator

    # ç¡®ä¿ SHAP èƒ½å¤Ÿè¯†åˆ«ç‰¹å¾å
    # XGBoost åœ¨ SHAP ä¸­æœ‰æ—¶éœ€è¦æ˜¾å¼æŒ‡å®šç‰¹å¾å
    explainer = shap.TreeExplainer(best_xgb)
    shap_values = explainer.shap_values(X_test_l)

    plt.figure(figsize=(12, 8))
    # ä½¿ç”¨ beeswarm å›¾å¯ä»¥æ›´ç›´è§‚åœ°çœ‹åˆ°ç‰¹å¾å¯¹ç»“æœçš„æ­£è´Ÿå½±å“
    shap.summary_plot(shap_values, X_test_l, show=False, max_display=15)
    plt.title("SHAP Feature Importance (XGBoost)", fontsize=14)
    plt.tight_layout()
    plt.savefig(os.path.join(SAVE_DIR_FIG, "XGB_SHAP_Beeswarm.png"), bbox_inches='tight')
    print("âœ… SHAP Beeswarm plot saved.")

except Exception as e:
    print(f"âš ï¸ SHAP plotting failed: {e}")

# ä¿å­˜æœ€ç»ˆçš„æ¨¡å‹å’Œç‰¹å¾åˆ—è¡¨
joblib.dump(results_store["XGBoost"]["model"], os.path.join(SAVE_DIR_MODEL, "calibrated_xgb.pkl"))
joblib.dump(scaler_pipe, os.path.join(SAVE_DIR_MODEL, "scaler_pipe.pkl"))
# è®°å½• LASSO é€‰ä¸­çš„ç‰¹å¾åï¼Œæ–¹ä¾¿ eICU éªŒè¯æ—¶å¯¹é½
with open(os.path.join(SAVE_DIR_MODEL, "selected_features.txt"), "w") as f:
    for feat in selected_feats:
        f.write(f"{feat}\n")

print(f"\nâœ… å…¨éƒ¨å®Œæˆã€‚æ•°æ®ä¿å­˜åœ¨: {SAVE_DIR_DATA}")
