import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import roc_auc_score, roc_curve
from imblearn.over_sampling import SMOTE
import shap

# =========================================================
# 0. é…ç½®ä¸è·¯å¾„
# =========================================================
RANDOM_STATE = 42
SAVE_DIR = "../figures/nosofa_comparison"
os.makedirs(SAVE_DIR, exist_ok=True)

# =========================================================
# 1. åŠ è½½æ•°æ®
# =========================================================
df = pd.read_csv("../data/ap_final_analysis_cohort.csv")
print(f"Total cohort size: {df.shape[0]}")

# =========================================================
# 2. ç‰¹å¾å®šä¹‰ä¸ä¸¥æ ¼æ’é™¤é€»è¾‘
# =========================================================
TARGET = "pof"

# 1. åŸºç¡€æ’é™¤ï¼šç»“å±€å˜é‡ã€IDã€æ—¶é—´æˆ³ã€ä»¥åŠéä¸´åºŠå¹²æ‰°å˜é‡ï¼ˆRace, Insuranceï¼‰
BASE_EXCLUDE = [
    TARGET, "resp_pof", "cv_pof", "renal_pof", "mortality_28d",
    "sofa_score", "apsiii", "sapsii", "oasis", "lods",
    "mechanical_vent_flag", "vaso_flag", "los",
    "stay_id", "hadm_id", "subject_id",
    "intime", "admittime", "dischtime", "deathtime", "dod",
    "race", "insurance", "language" 
]

# 2. æ•æ„Ÿæ€§æ’é™¤åˆ—è¡¨ (è‚¾åŠŸèƒ½æŒ‡æ ‡)
SENSITIVITY_EXCLUDE = [
    "creatinine_min", "creatinine_max", "bun_min", "bun_max", "chloride_min", "chloride_max"
]

# ---- å¤„ç†ç±»åˆ«å˜é‡ï¼šå…ˆåˆ é™¤ ID/æ—¶é—´æˆ³å†åš One-Hotï¼Œé˜²æ­¢ç‰¹å¾çˆ†ç‚¸ ----
df_filtered = df.drop(columns=[c for c in BASE_EXCLUDE if c in df.columns])
df_numeric = pd.get_dummies(df_filtered, drop_first=True)

# 3. å®šä¹‰å®éªŒç»„
all_clinical_features = df_numeric.columns.tolist()
sensitivity_features = [c for c in all_clinical_features if c not in SENSITIVITY_EXCLUDE]

experiments = {
    "Main_Analysis": all_clinical_features,
    "Sensitivity_No_Renal": sensitivity_features
}

# =========================================================
# 3. è‡ªåŠ¨åŒ–å®éªŒå¾ªç¯
# =========================================================
exp_results = {}

for exp_name, feature_list in experiments.items():
    print(f"\n{'='*50}")
    print(f"ğŸš€ Running Experiment: {exp_name}")
    print(f"Initial Feature Count: {len(feature_list)}")

    X_exp = df_numeric[feature_list]
    y_exp = df[TARGET]

    # 1. æ‹†åˆ†æ•°æ®é›†
    X_train, X_test, y_train, y_test = train_test_split(
        X_exp, y_exp, test_size=0.30, stratify=y_exp, random_state=RANDOM_STATE
    )

    # 2. é¢„å¤„ç†ï¼šå¡«è¡¥ä¸æ ‡å‡†åŒ–
    preprocess = Pipeline([
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler())
    ])
    
    X_train_scaled = pd.DataFrame(preprocess.fit_transform(X_train), columns=X_train.columns, index=X_train.index)
    X_test_scaled = pd.DataFrame(preprocess.transform(X_test), columns=X_test.columns, index=X_test.index)

    # 3. LASSO ç‰¹å¾ç­›é€‰
    print("--- Running LASSO Selection ---")
    lasso = LogisticRegressionCV(
        Cs=10, cv=5, penalty="l1", solver="saga", scoring="roc_auc", 
        max_iter=5000, n_jobs=-1, random_state=RANDOM_STATE
    )
    lasso.fit(X_train_scaled, y_train)
    
    coef = pd.Series(lasso.coef_[0], index=X_train.columns)
    selected_features = coef[coef != 0].index.tolist()
    print(f"LASSO selected {len(selected_features)} features")

    X_train_lasso = X_train_scaled[selected_features]
    X_test_lasso = X_test_scaled[selected_features]

    # 4. SMOTE (ä»…é’ˆå¯¹è®­ç»ƒé›†)
    smote = SMOTE(random_state=RANDOM_STATE)
    X_train_res, y_train_res = smote.fit_resample(X_train_lasso, y_train)

    # 5. æ¨¡å‹è®­ç»ƒä¸è¯„ä¼° (é’ˆå¯¹å°æ ·æœ¬å¾®è°ƒåçš„ XGBoost)
    print("--- Training Tuned XGBoost Model ---")
    model = XGBClassifier(
        n_estimators=500,       # å¢åŠ æ ‘çš„æ•°é‡
        max_depth=3,            # é™ä½æ·±åº¦é˜²æ­¢è¿‡æ‹Ÿåˆ
        learning_rate=0.02,     # é™ä½æ­¥é•¿æé«˜æ³›åŒ–
        gamma=1.0,              # å¢åŠ åˆ†è£‚é—¨æ§›
        subsample=0.7,          # æ ·æœ¬æ‰°åŠ¨
        colsample_bytree=0.7,   # ç‰¹å¾æ‰°åŠ¨
        min_child_weight=5,     # é™åˆ¶å¶å­èŠ‚ç‚¹æœ€å°æƒé‡
        reg_lambda=2.0,         # L2 æ­£åˆ™åŒ–
        eval_metric="logloss",
        random_state=RANDOM_STATE
    )
    model.fit(X_train_res, y_train_res)
    
    y_prob = model.predict_proba(X_test_lasso)[:, 1]
    auc_score = roc_auc_score(y_test, y_prob)
    print(f"{exp_name} XGBoost AUC: {auc_score:.4f}")

    # å­˜å‚¨ç»“æœç”¨äºå¯¹æ¯”
    exp_results[exp_name] = {
        "y_true": y_test,
        "y_prob": y_prob,
        "auc": auc_score,
        "selected_features": selected_features,
        "model": model,
        "X_test": X_test_lasso
    }

    # 6. ä¿å­˜è¯¥ç»„å®éªŒçš„ç‰¹å¾ç³»æ•°
    coef[selected_features].sort_values(ascending=False).to_csv(
        os.path.join(SAVE_DIR, f"Features_{exp_name}.csv")
    )

# =========================================================
# 4. æœ€ç»ˆå¯¹æ¯”å¯è§†åŒ–
# =========================================================

# 1. ç»¼åˆ ROC æ›²çº¿
plt.figure(figsize=(8, 7))
for name, data in exp_results.items():
    fpr, tpr, _ = roc_curve(data["y_true"], data["y_prob"])
    plt.plot(fpr, tpr, label=f"{name} (AUC = {data['auc']:.3f})")

# åŠ å…¥ SOFA Benchmark å‚ç…§
sofa_test = df.loc[exp_results["Main_Analysis"]["y_true"].index, "sofa_score"]
fpr_s, tpr_s, _ = roc_curve(exp_results["Main_Analysis"]["y_true"], sofa_test)
plt.plot(fpr_s, tpr_s, 'k--', alpha=0.5, label=f"SOFA Benchmark (AUC = {roc_auc_score(exp_results['Main_Analysis']['y_true'], sofa_test):.3f})")

plt.plot([0, 1], [0, 1], color='gray', linestyle=':', lw=1)
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Comparison: Main vs Sensitivity Analysis")
plt.legend(loc="lower right")
plt.tight_layout()
plt.savefig(os.path.join(SAVE_DIR, "Final_Comparison_ROC.png"), dpi=300)

# 2. SHAP å¯¹æ¯” (é’ˆå¯¹ Main Analysis)
print("\n--- Generating SHAP for Main Analysis ---")
best_data = exp_results["Main_Analysis"]
explainer = shap.TreeExplainer(best_data["model"])
shap_values = explainer.shap_values(best_data["X_test"])

plt.figure()
shap.summary_plot(shap_values, best_data["X_test"], show=False)
plt.title("SHAP Summary: Main Analysis (Tuned Model)")
plt.savefig(os.path.join(SAVE_DIR, "SHAP_Main_Analysis.png"), dpi=300)

print(f"\nâœ… All experiments completed. Figures saved to {SAVE_DIR}")
