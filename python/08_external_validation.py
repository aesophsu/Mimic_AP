import os
import pandas as pd
import numpy as np
import joblib
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, roc_auc_score, brier_score_loss, precision_recall_curve, auc

# =========================================================
# 1. é…ç½®ä¸è·¯å¾„
# =========================================================
BASE_DIR = ".."
DATA_DIR = os.path.join(BASE_DIR, "data/cleaned")
MODELS_DIR = os.path.join(BASE_DIR, "models")
SAVE_DIR = os.path.join(BASE_DIR, "results/validation")

if not os.path.exists(SAVE_DIR):
    os.makedirs(SAVE_DIR)

def run_module_08_multi_model_eval():
    print("="*85)
    print("ğŸ† æ¨¡å— 08: å¤šæ¨¡å‹å¤–éƒ¨éªŒè¯ (MIMIC -> eICU)")
    print("="*85)

    targets = ['pof', 'composite_outcome', 'mortality_28d']
    
    # è®¾ç½®ç»˜å›¾é£æ ¼
    plt.style.use('seaborn-v0_8-whitegrid')
    # æ¯ä¸€è¡Œä»£è¡¨ä¸€ä¸ªç»“å±€ï¼Œç»˜åˆ¶å…¶æ‰€æœ‰æ¨¡å‹çš„ ROC
    fig, axes = plt.subplots(1, 3, figsize=(22, 7), dpi=150)

    # ç”¨äºæ±‡æ€»æ‰€æœ‰æŒ‡æ ‡çš„åˆ—è¡¨
    performance_metrics = []

    for i, target in enumerate(targets):
        print(f"\nğŸš€ æ­£åœ¨éªŒè¯ç»“å±€: {target.upper()}")
        
        # 1. åŠ è½½ eICU æ•°æ®
        eicu_path = os.path.join(DATA_DIR, f"eicu_for_model_{target}.csv")
        if not os.path.exists(eicu_path):
            print(f"  âš ï¸ è·³è¿‡: æ‰¾ä¸åˆ°æ•°æ® {eicu_path}")
            continue
            
        df_eicu = pd.read_csv(eicu_path)
        X_eicu = df_eicu.drop('target', axis=1)
        y_eicu = df_eicu['target']

        # 2. åŠ è½½æ¨¡å‹å­—å…¸
        model_dict_path = os.path.join(MODELS_DIR, f"all_models_{target}.pkl")
        if not os.path.exists(model_dict_path):
            print(f"  âš ï¸ è·³è¿‡: æ‰¾ä¸åˆ°æ¨¡å‹åŒ… {model_dict_path}")
            continue
            
        models_dict = joblib.load(model_dict_path)
        
        # 3. éå†å­—å…¸ä¸­çš„æ¯ä¸€ä¸ªæ¨¡å‹è¿›è¡Œè¯„ä¼°
        for algo_name, model in models_dict.items():
            try:
                # é¢„æµ‹æ¦‚ç‡
                y_prob = model.predict_proba(X_eicu.values)[:, 1]
                
                # è®¡ç®—æŒ‡æ ‡
                auc_score = roc_auc_score(y_eicu, y_prob)
                brier = brier_score_loss(y_eicu, y_prob)
                
                # è®¡ç®— PR-AUC
                prec, rec, _ = precision_recall_curve(y_eicu, y_prob)
                pr_auc = auc(rec, prec)
                
                # ä¿å­˜ç»“æœ
                performance_metrics.append({
                    'Target': target,
                    'Algorithm': algo_name,
                    'ROC-AUC': auc_score,
                    'PR-AUC': pr_auc,
                    'Brier': brier
                })
                
                # 4. ç»˜åˆ¶ ROC æ›²çº¿
                fpr, tpr, _ = roc_curve(y_eicu, y_prob)
                axes[i].plot(fpr, tpr, lw=1.5, label=f'{algo_name} (AUC={auc_score:.3f})')
                
                print(f"  âœ… {algo_name:<20} | AUC: {auc_score:.4f} | Brier: {brier:.4f}")
                
            except Exception as e:
                print(f"  âŒ è¯„ä¼° {algo_name} æ—¶å‡ºé”™: {e}")

        # è®¾ç½®å­å›¾æ ¼å¼
        axes[i].plot([0, 1], [0, 1], color='gray', linestyle='--', alpha=0.5)
        axes[i].set_title(f'Outcome: {target.upper()}', fontsize=14, fontweight='bold')
        axes[i].set_xlabel('False Positive Rate')
        axes[i].set_ylabel('True Positive Rate')
        axes[i].legend(loc="lower right", fontsize=9, frameon=True)

    # 5. ä¿å­˜æŒ‡æ ‡æ±‡æ€»è¡¨
    summary_df = pd.DataFrame(performance_metrics)
    summary_path = os.path.join(SAVE_DIR, "external_validation_summary.csv")
    summary_df.to_csv(summary_path, index=False)
    
    plt.tight_layout()
    plt.savefig(os.path.join(SAVE_DIR, "multi_model_external_roc.png"))
    print(f"\nğŸ“Š æ‰€æœ‰æ¨¡å‹éªŒè¯å®Œæˆï¼")
    print(f"ğŸ“‚ æŒ‡æ ‡æ±‡æ€»å·²ä¿å­˜è‡³: {summary_path}")
    print(f"ğŸ–¼ï¸ ROC æ›²çº¿å›¾å·²ä¿å­˜è‡³: {os.path.join(SAVE_DIR, 'multi_model_external_roc.png')}")
    plt.show()

if __name__ == "__main__":
    run_module_08_multi_model_eval()
