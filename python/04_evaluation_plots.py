import os
import pandas as pd
import numpy as np
import joblib
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc, brier_score_loss
import shap
import warnings

# å±è”½ä¸å¿…è¦çš„ UserWarning å¹²æ‰°
warnings.filterwarnings('ignore', category=UserWarning)

# é…ç½®è·¯å¾„
MODEL_DIR = "../models"
FIG_DIR = "../figures"
if not os.path.exists(FIG_DIR): os.makedirs(FIG_DIR)

def calculate_net_benefit(y_true, y_prob, thresh):
    y_pred = (y_prob >= thresh).astype(int)
    tp = np.sum((y_pred == 1) & (y_true == 1))
    fp = np.sum((y_pred == 1) & (y_true == 0))
    n = len(y_true)
    if thresh >= 1.0 or thresh <= 0: return 0
    return (tp / n) - (fp / n) * (thresh / (1 - thresh))

def run_module_04_debug_version():
    print("="*70)
    print("ðŸš€ è¿è¡Œæ¨¡å— 04 å¢žå¼ºå®¡è®¡ç‰ˆ: å¯è§†åŒ–ä¸Ž SHAP è§£é‡Š")
    print("="*70)

    # 1. åŠ è½½èµ„äº§å¹¶æ‰“å°å®¡è®¡ä¿¡æ¯
    print("ðŸ“‚ [Step 1/4] æ­£åœ¨åŠ è½½æ¨¡åž‹ä¸Žæ•°æ®èµ„äº§...")
    try:
        all_models = joblib.load(os.path.join(MODEL_DIR, "all_models.pkl"))
        selected_features = joblib.load(os.path.join(MODEL_DIR, "selected_features.pkl"))
        
        X_test, y_test = joblib.load(os.path.join(MODEL_DIR, "test_data_main.pkl"))
        X_test_np = X_test.values if hasattr(X_test, 'values') else X_test

        # [æ–°å¢ž] åŠ è½½äºšç»„æ•°æ®ç”¨äºŽå¯¹æ¯”å®¡è®¡
        X_sub, y_sub = joblib.load(os.path.join(MODEL_DIR, "test_data_sub.pkl"))
        X_sub_np = X_sub.values if hasattr(X_sub, 'values') else X_sub

        print(f"   âœ… åŠ è½½æˆåŠŸ: åŒ…å« {len(all_models)} ä¸ªæ¨¡åž‹")
        print(f"   âœ… ç‰¹å¾åˆ—è¡¨: {selected_features}")
        print(f"   âœ… æµ‹è¯•é›†ç»´åº¦: {X_test_np.shape}, POF æµè¡ŒçŽ‡: {np.mean(y_test):.2%}")
    except Exception as e:
        print(f"   âŒ åŠ è½½å¤±è´¥: {e}")
        return

    # --------------------------------------------------------
    # [å›¾ 1] å…¨æ¨¡åž‹ ROC å¯¹æ¯”
    # --------------------------------------------------------
    print("\nðŸ“ˆ [Step 2/4] æ­£åœ¨ç»˜åˆ¶å¤šæ¨¡åž‹ ROC æ›²çº¿å¹¶å®¡è®¡ AUC...")
    plt.figure(figsize=(9, 8))
    # --------------------------------------------------------
    # [Step 2] åŒæ­¥æ¨¡å— 03 çš„å®¡è®¡æ•°æ®
    # --------------------------------------------------------
    # å¡«å…¥æ¨¡å— 03 æ‰“å°çš„ Main AUC (95% CI)
    ci_data = {
        "XGBoost": "0.831 (0.771-0.882)",
        "SVM": "0.839 (0.782-0.888)",
        "Random Forest": "0.834 (0.777-0.885)",
        "Logistic Regression": "0.833 (0.775-0.884)",
        "Decision Tree": "0.818 (0.760-0.873)"
    }

    # [æ–°å¢ž] å¡«å…¥æ¨¡å— 03 æ‰“å°çš„ No-Renal AUC (95% CI)
    sub_ci_data = {
        "XGBoost": "0.752 (0.645-0.853)",
        "SVM": "0.774 (0.656-0.880)",
        "Random Forest": "0.760 (0.647-0.861)",
        "Logistic Regression": "0.768 (0.656-0.862)",
        "Decision Tree": "0.745 (0.631-0.852)"
    }

    for name, clf in all_models.items():
        # å¼ºåˆ¶ä½¿ç”¨ numpy æ•°ç»„é¢„æµ‹ï¼Œæ¶ˆé™¤è­¦å‘Š
        y_prob = clf.predict_proba(X_test_np)[:, 1]
        fpr, tpr, _ = roc_curve(y_test, y_prob)
        roc_auc = auc(*roc_curve(y_test, y_prob)[:2])
    
        # [æ–°å¢ž] è®¡ç®—äºšç»„æ€§èƒ½
        y_prob_sub = clf.predict_proba(X_sub_np)[:, 1]
        roc_auc_sub = auc(*roc_curve(y_sub, y_prob_sub)[:2])
    
        # ä¿®æ”¹æ‰“å°ä¿¡æ¯ï¼Œå¢žåŠ  Sub-AUC å®¡è®¡
        print(f"   ðŸ” æ¨¡åž‹å®¡è®¡: {name:<20} | Test AUC: {roc_auc:.4f} | Sub-AUC: {roc_auc_sub:.4f}")
    
        display_label = f"{name}: {ci_data.get(name, f'{roc_auc:.3f}')}"
        plt.plot(fpr, tpr, lw=2, label=display_label)

    plt.plot([0, 1], [0, 1], 'k--', alpha=0.2)
    plt.xlabel('False Positive Rate', fontsize=12)
    plt.ylabel('True Positive Rate', fontsize=12)
    plt.title('Predictive Performance Comparison', fontsize=14, fontweight='bold')
    plt.legend(loc='lower right', fontsize=9)
    plt.grid(alpha=0.2)
    plt.savefig(os.path.join(FIG_DIR, "01_ROC_Comparison.png"), dpi=300)
    plt.close()

    # --------------------------------------------------------
    # [å›¾ 2] SHAP è§£é‡Š (é’ˆå¯¹ XGBoost)
    # --------------------------------------------------------
    print("\nðŸ§ª [Step 3/4] æ­£åœ¨ç”Ÿæˆ XGBoost SHAP è§£é‡Š (é’ˆå¯¹ä¿®æ­£æ ‡ç­¾åŽçš„å®¡è®¡)...")
    try:
        xgb_calibrated = all_models['XGBoost']
        xgb_raw = xgb_calibrated.calibrated_classifiers_[0].estimator
        
        explainer = shap.TreeExplainer(xgb_raw)
        # ä½¿ç”¨ Numpy æ•°ç»„ä»¥ç¡®ä¿ç‰¹å¾å¯¹åº”æ­£ç¡®
        shap_values = explainer.shap_values(X_test_np)
        
        print(f"   âœ… SHAP è®¡ç®—å®Œæˆã€‚SHAP æ•°ç»„ç»´åº¦: {np.shape(shap_values)}")
        
        plt.figure(figsize=(12, 8))
        shap.summary_plot(
            shap_values, 
            X_test_np, 
            feature_names=selected_features, 
            show=False,
            plot_type="dot"
        )
        plt.title('SHAP Feature Importance: Drivers of POF Risk', fontsize=14)
        plt.tight_layout()
        plt.savefig(os.path.join(FIG_DIR, "02_SHAP_Summary.png"), dpi=300)
        plt.close()
    except Exception as e:
        print(f"   âš ï¸ SHAP ç”Ÿæˆè·³è¿‡: {e}")

    # --------------------------------------------------------
    # Step 4: å…¨æ¨¡åž‹ DCA ä¸´åºŠä»·å€¼å®¡è®¡ (ä¿®å¤ç´¢å¼•é”™è¯¯å¹¶å…¨é‡åŒ–)
    # --------------------------------------------------------
    print("\nâš–ï¸ [Step 4/4] æ­£åœ¨æ‰§è¡Œå…¨æ¨¡åž‹ DCA ä¸´åºŠä»·å€¼å®¡è®¡...")
    plt.figure(figsize=(10, 8))
    thresholds = np.arange(0.01, 0.81, 0.01)
    
    # åŸºç¡€å‚ç…§çº¿: Treat All (æ‰€æœ‰äººéƒ½è§†ä¸ºé«˜å±)
    prev = np.mean(y_test)
    nb_all = [prev - (1 - prev) * (t / (1 - t)) for t in thresholds]
    
    model_windows = {}
    colors = ['#d62728', '#1f77b4', '#2ca02c', '#ff7f0e', '#9467bd']

    for (name, clf), color in zip(all_models.items(), colors):
        y_prob = clf.predict_proba(X_test_np)[:, 1]
        nb_model = [calculate_net_benefit(y_test, y_prob, t) for t in thresholds]
        
        # ç²¾ç¡®è®¡ç®—èŽ·ç›Šçª—å£: Net Benefit > Treat All ä¸” Net Benefit > 0
        better_than_all = [t for t, nb, nba in zip(thresholds, nb_model, nb_all) if nb > nba and nb > 0]
        
        if better_than_all:
            win_min, win_max = min(better_than_all), max(better_than_all)
            window_str = f"{win_min:.1%} - {win_max:.1%}"
            model_windows[name] = window_str
            print(f"   âœ… {name:<20} | èŽ·ç›Šçª—å£: {window_str}")
        else:
            model_windows[name] = "No Benefit"
            print(f"   âš ï¸ {name:<20} | æœªæ£€æµ‹åˆ°èŽ·ç›ŠåŒºé—´")

        plt.plot(thresholds, nb_model, lw=2, color=color, label=f"{name} ({model_windows[name]})")

    # ç»˜åˆ¶å‚è€ƒè™šçº¿
    plt.plot(thresholds, nb_all, color='black', linestyle=':', alpha=0.4, label='Treat All')
    plt.axhline(y=0, color='gray', lw=1, label='Treat None')
    
    plt.ylim(-0.05, prev + 0.1)
    plt.xlim(0, 0.8)
    plt.xlabel('Risk Threshold Probability (Cut-off)')
    plt.ylabel('Net Benefit')
    plt.title('Decision Curve Analysis: Comparative Utility', fontsize=14, fontweight='bold')
    plt.legend(loc='upper right', fontsize=9)
    plt.grid(alpha=0.2)
    plt.savefig(os.path.join(FIG_DIR, "03_DCA_Full_Comparison.png"), dpi=300)
    plt.close()

    # --------------------------------------------------------
    # æ€»ç»“è¾“å‡º (Table 2 ç»ˆæžç‰ˆ)
    # --------------------------------------------------------
    print("\n" + "="*115)
    print(f"{'Algorithm':<20} | {'Main AUC (95% CI)':<25} | {'No-Renal AUC (95% CI)':<25} | {'DCA Window':<15}")
    print("-" * 115)
    for name in all_models.keys():
        main_val = ci_data.get(name, "N/A")
        sub_val = sub_ci_data.get(name, "N/A")
        window = model_windows.get(name, "N/A")
        print(f"{name:<20} | {main_val:<25} | {sub_val:<25} | {window:<15}")
    print("="*115)
    print(f"ðŸŽ‰ æ¨¡å— 04 è¿è¡ŒæˆåŠŸï¼å›¾è¡¨ä½äºŽ: {FIG_DIR}")
    
if __name__ == "__main__":
    run_module_04_debug_version()
