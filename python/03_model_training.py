import os
import pandas as pd
import numpy as np
import joblib
import optuna

# æœºå™¨å­¦ä¹ æ ¸å¿ƒåº“
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.linear_model import LogisticRegression, LassoCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.calibration import CalibratedClassifierCV
from sklearn.metrics import roc_auc_score, brier_score_loss
import matplotlib.pyplot as plt
from sklearn.linear_model import Lasso
from sklearn.metrics import roc_curve, roc_auc_score, brier_score_loss
from sklearn.calibration import calibration_curve # å°†å®ƒä» calibration æ¨¡å—å¯¼å…¥# å±è”½è­¦å‘Š
from sklearn.utils import resample # ç¡®ä¿åœ¨é¡¶éƒ¨æˆ–æ­¤å¤„å¯¼å…¥
import warnings
warnings.filterwarnings('ignore')
optuna.logging.set_verbosity(optuna.logging.WARNING)

# =========================================================
# 1. é…ç½®ä¸è·¯å¾„
# =========================================================
BASE_DIR = ".."
INPUT_PATH = os.path.join(BASE_DIR, "data/cleaned/mimic_for_model.csv")
SAVE_DIR = os.path.join(BASE_DIR, "models")
FIG_DIR = os.path.join(BASE_DIR, "figures") # æ–°å¢ï¼šå›¾ç‰‡ä¿å­˜ç›®å½•
for d in [SAVE_DIR, FIG_DIR]:
    if not os.path.exists(d):
        os.makedirs(d)

FINAL_SUMMARY_STORAGE = []
def run_module_03_all_outcomes():
    """
    æ ¸å¿ƒæ§åˆ¶å‡½æ•°ï¼šå¾ªç¯æ‰§è¡Œä¸åŒç»“å±€çš„åˆ†æï¼Œå¹¶æ±‡æ€»æœ€ç»ˆæŠ¥è¡¨
    """
    global FINAL_SUMMARY_STORAGE
    FINAL_SUMMARY_STORAGE = [] # æ¸…ç©ºç¼“å­˜
    
    # ç»“å±€åˆ—è¡¨ï¼šç°åœ¨åŒ…å«äº†æ­»äº¡ç‡æ¨¡å‹
    targets = ['pof', 'composite_outcome', 'mortality_28d']
    
    for current_target in targets:
        print(f"\n\n{'='*30} æ­£åœ¨åˆ†æç»“å±€: {current_target.upper()} {'='*30}")
        # è·å–è¯¥ç»“å±€ä¸‹çš„æ‰€æœ‰æ¨¡å‹æ€§èƒ½æŒ‡æ ‡
        target_results = train_pipeline(current_target)
        FINAL_SUMMARY_STORAGE.extend(target_results)

    # --- æ ¸å¿ƒä¼˜åŒ–ï¼šç”Ÿæˆå…¨å±€æ€§èƒ½æ±‡æ€»è¡¨ ---
    summary_df = pd.DataFrame(FINAL_SUMMARY_STORAGE)
    
    # æŒ‰ç…§ç»“å±€å’Œ AUC æ’åºï¼Œæ–¹ä¾¿æŸ¥çœ‹å“ªä¸ªæ¨¡å‹æœ€å¼º
    summary_df = summary_df.sort_values(by=['Outcome', 'Main AUC'], ascending=[True, False])
    
    # ä¿å­˜æ±‡æ€»è¡¨
    summary_save_path = os.path.join(SAVE_DIR, "all_outcomes_performance_summary.csv")
    summary_df.to_csv(summary_save_path, index=False)
    
    print("\n" + "#"*60)
    print("ğŸ† æ‰€æœ‰ç»“å±€åˆ†æå®Œæˆï¼æœ€ç»ˆæ€§èƒ½æ±‡æ€»è¡¨å·²ç”Ÿæˆï¼š")
    print(f"ğŸ“ è·¯å¾„: {summary_save_path}")
    print("#"*60)
    print(summary_df.to_string(index=False))

def train_pipeline(target):
    print("="*60)
    print("ğŸš€ è¿è¡Œç»ˆæé‡æ„æ¨¡å— 03: 5 ç§æ¨¡å‹ç«èµ› + åŠ¨æ€å¯¹æ•°å¤„ç†")
    print("="*60)
    
    if not os.path.exists(INPUT_PATH):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶ {INPUT_PATH}")
        return
        
    df = pd.read_csv(INPUT_PATH)

    # =========================================================
    # 2. ç‰¹å¾æ¸…æ´—ä¸é¢„å¤„ç† (å…³é”®ï¼šä¿®å¤æ–‡æœ¬åˆ—æŠ¥é”™)
    # =========================================================
    print(f"\nğŸ“‹ åŸå§‹æ•°æ®æ¢æµ‹: {df.shape[0]} è¡Œ, {df.shape[1]} åˆ—")
    print(f"{'Feature Name':<25} | {'Missing%':<10} | {'Median':<10} | {'Mean':<10} | {'Max':<10}")
    print("-" * 75)
    
    for col in df.columns:
        if pd.api.types.is_numeric_dtype(df[col]):
            series = df[col].dropna() # æ’é™¤ç©ºå€¼è¿›è¡Œè®¡ç®—
            missing = df[col].isnull().mean() * 100
            med = series.median() if not series.empty else 0
            mean = series.mean() if not series.empty else 0
            v_max = series.max() if not series.empty else 0
            print(f"{col:<25} | {missing:>8.2f}% | {med:>10.2f} | {mean:>10.2f} | {v_max:>10.2f}")
            
    if 'gender' in df.columns:
        df['gender'] = df['gender'].map({'M': 1, 'F': 0})

    outcome_cols = [
        'pof', 'mortality_28d', 'composite_outcome', 
        'renal_pof', 'resp_pof', 'cv_pof'
    ]    
    drop_list = outcome_cols + [
        'subgroup_no_renal', 'hosp_mortality', 'overall_mortality', 'stay_id'
    ]
    

    # ğŸ›¡ï¸ è‡ªåŠ¨å‰”é™¤éæ•°å€¼ç‰¹å¾ (å¤„ç† ValueError: could not convert string to float)
    text_cols = df.select_dtypes(include=['object']).columns.tolist()
    final_drop = list(set(drop_list + text_cols))
    print(f"ğŸ—‘ï¸ è‡ªåŠ¨å‰”é™¤æ³„éœ²/éæ•°å€¼ç‰¹å¾: {text_cols}")
    
    X = df.drop(columns=[c for c in final_drop if c in df.columns])
    y = df[target]
    subgroup_flag = df['subgroup_no_renal']

    # å¤„ç†æ— ç©·å¤§å¹¶ç¡®ä¿æ•°å€¼åŒ–
    X = X.replace([np.inf, -np.inf], np.nan).astype(float)
    
    # åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†
    X_train, X_test, y_train, y_test, sub_train, sub_test = train_test_split(
        X, y, subgroup_flag, test_size=0.2, random_state=42, stratify=y
    )
    # --- [ä¿®æ”¹ä½ç½® A]: åˆ’åˆ†åçš„åˆ†å¸ƒå®¡è®¡ ---
    print(f"\nğŸ›¡ï¸ äºšç»„åˆ†å¸ƒå¹³è¡¡å®¡è®¡:")
    print(f"  Train Set: n={len(y_train)}, No-Renal Subgroup={sub_train.sum()} ({sub_train.sum()/len(sub_train):.1%})")
    print(f"  Test Set:  n={len(y_test)}, No-Renal Subgroup={sub_test.sum()} ({sub_test.sum()/len(sub_test):.1%})")
    # =========================================================
    # 3. ğŸ§ª æ ¸å¿ƒä¿®æ­£ï¼šåŠ¨æ€ Log1p è½¬æ¢ (æ•‘èµçº¿æ€§æ¨¡å‹)
    # =========================================================
    skewed_cols = ['creatinine_max', 'creatinine_min', 'bun_max', 'bun_min',
                   'wbc_max', 'wbc_min', 'glucose_max', 'glucose_min',
                   'lipase_max', 'lactate_max',
                   'alt_max', 'ast_max', 'bilirubin_total_max', 
                   'alp_max', 'inr_max', 'rdw_max']
    existing_skewed = [c for c in skewed_cols if c in X_train.columns]
    print(f"\nğŸ”„ æ­£åœ¨æ‰§è¡ŒåŠ¨æ€ Log1p è½¬æ¢ä¸é‡çº§å®¡è®¡...")
    for col in existing_skewed:
        # åœ¨è½¬æ¢å‰è®°å½•ä¸­ä½æ•°ï¼Œç”¨äºè·¨åº“ä¸€è‡´æ€§æ¯”å¯¹
        train_med = X_train[col].median()
        test_med = X_test[col].median()
        print(f"  [Audit] {col:<20}: train_median={train_med:>8.2f}, test_median={test_med:>8.2f}")
        
        # æ‰§è¡Œå¸¦è£å‰ªçš„å¯¹æ•°è½¬æ¢
        X_train[col] = np.log1p(X_train[col].clip(lower=0))
        X_test[col] = np.log1p(X_test[col].clip(lower=0))

    # =========================================================
    # 4. å¢å¼ºå‹å¤šé‡æ’è¡¥ (MICE) & æ ‡å‡†åŒ–
    # =========================================================
    print("ğŸ§ª æ­£åœ¨æ‰§è¡Œæ·±åº¦æ’è¡¥ (MICE)...")
    mice_imputer = IterativeImputer(max_iter=20, random_state=42, initial_strategy='median')
    scaler = StandardScaler()

    X_train_imp = mice_imputer.fit_transform(X_train)
    # --- [ä¿®æ”¹ä½ç½® C]: æ’è¡¥åçš„è´¨é‡å®¡è®¡ ---
    # ç»Ÿè®¡ç¼ºå¤±ç‡è¶…è¿‡ 40% çš„ç‰¹å¾
    high_missing = X_train.columns[X_train.isnull().mean() > 0.4].tolist()
    if high_missing:
        print(f"âš ï¸ æ’è¡¥é£é™©æç¤º: ä»¥ä¸‹å˜é‡ç¼ºå¤±ç‡ > 40%ï¼ŒMICE æ’è¡¥å¯èƒ½å¼•å…¥å™ªå£°:\n  {high_missing}")
    X_train_std = scaler.fit_transform(X_train_imp)

    X_test_imp = mice_imputer.transform(X_test)
    X_test_std = scaler.transform(X_test_imp)

    # ä¿å­˜é¢„å¤„ç†èµ„äº§
    joblib.dump(scaler, os.path.join(SAVE_DIR, f"scaler_{target}.pkl"))
    joblib.dump(mice_imputer, os.path.join(SAVE_DIR, f"mice_imputer_{target}.pkl"))
    joblib.dump(existing_skewed, os.path.join(SAVE_DIR, f"skewed_cols_{target}.pkl"))

    # =========================================================
    # 5. LASSO ç‰¹å¾é™ç»´ (Top 12) - å­¦æœ¯å¢å¼ºç‰ˆ
    # =========================================================
    print("ğŸ§ª æ­£åœ¨ç²¾é€‰æè‡´æ ¸å¿ƒç‰¹å¾ (Top 12)å¹¶ç”Ÿæˆå­¦æœ¯å›¾è¡¨...")
    
    # æ‰§è¡Œ LassoCV
    lasso = LassoCV(cv=5, random_state=42, max_iter=20000).fit(X_train_std, y_train)
    
    # --- [è®¡ç®—ç»˜å›¾æ‰€éœ€æŒ‡æ ‡] ---
    alphas = lasso.alphas_
    log_alphas = np.log10(alphas)
    mse_mean = lasso.mse_path_.mean(axis=1)
    mse_std = lasso.mse_path_.std(axis=1)
    mse_se = mse_std / np.sqrt(lasso.mse_path_.shape[1]) # æ ‡å‡†è¯¯
    
    # æ‰¾åˆ° Min MSE å’Œ 1-SE ç‚¹
    idx_min = np.argmin(mse_mean)
    target_mse = mse_mean[idx_min] + mse_se[idx_min]
    # 1-SE ç‚¹ï¼šåœ¨ idx_min ä¹‹åï¼ˆæ›´ç®€å•çš„æ¨¡å‹ä¸­ï¼‰å¯»æ‰¾æœ€åä¸€ä¸ªæ»¡è¶³ MSE <= target_mse çš„ç´¢å¼•
    idx_1se = np.where(mse_mean <= target_mse)[0][0] 

    # è·å–ç‰¹å¾è·¯å¾„ç”¨äºé¡¶éƒ¨è®¡æ•°
    from sklearn.linear_model import lasso_path
    _, coefs_path, _ = lasso_path(X_train_std, y_train, alphas=alphas)
    active_counts = np.sum(coefs_path != 0, axis=0)

    # --- [ç»˜åˆ¶å­¦æœ¯é£æ ¼ Lasso CV å›¾] ---
    plt.figure(figsize=(10, 7), dpi=300)
    ax1 = plt.gca()
    
    # 1. ç»˜åˆ¶è¯¯å·®æ£’ (Error Bars)
    ax1.errorbar(log_alphas, mse_mean, yerr=mse_se, fmt='o', color='red', 
                 ecolor='gray', elinewidth=1, capsize=2, mfc='red', ms=5, label='Cross-Validation Error')
    
    # 2. ç»˜åˆ¶ Min MSE çº¿ (è“) å’Œ 1-SE çº¿ (é»‘)
    ax1.axvline(log_alphas[idx_min], color='blue', linestyle='--', label=f'Min Error (n={active_counts[idx_min]})')
    ax1.axvline(log_alphas[idx_1se], color='black', linestyle='--', label=f'1-SE Rule (n={active_counts[idx_1se]})')

    ax1.set_xlabel(r'$\log_{10}(\alpha)$', fontsize=12)
    ax1.set_ylabel('Mean Squared Error (MSE)', fontsize=12)
    ax1.set_title('Lasso Variable Selection with 1-SE Rule', fontsize=14, fontweight='bold')
    ax1.legend(loc='upper left', fontsize=10)
    ax1.grid(alpha=0.3)

    # 3. æ·»åŠ é¡¶éƒ¨ç‰¹å¾è®¡æ•°è½´
    ax2 = ax1.twiny()
    ax2.set_xlim(ax1.get_xlim())
    tick_indices = np.linspace(0, len(log_alphas)-1, 10, dtype=int)
    ax2.set_xticks(log_alphas[tick_indices])
    ax2.set_xticklabels(active_counts[tick_indices])
    ax2.set_xlabel('Number of Non-zero Coefficients', fontsize=12, labelpad=10)

    plt.tight_layout()
    plt.savefig(os.path.join(FIG_DIR, f"Academic_Lasso_{target}.png"), dpi=300)
    plt.show()
    plt.close()

    # --- [ç‰¹å¾æå–ä¿æŒä¸å˜] ---
    coef_abs = np.abs(lasso.coef_)
    indices = np.argsort(coef_abs)[-12:] 
    selected_features = X.columns[indices].tolist()
    
    X_train_final = X_train_std[:, indices]
    X_test_final = X_test_std[:, indices]
    print(f"âœ… ç‰¹å¾ç²¾ç®€å®Œæˆ: {selected_features}")

    # =========================================================
    # 6. XGBoost Optuna è¶…å‚æ•°å¯»ä¼˜
    # =========================================================
    print("\nğŸ”¬ å¯åŠ¨ XGBoost è´å¶æ–¯å¯»ä¼˜ (Optuna)...")
    def objective(trial):
        param = {
            'n_estimators': trial.suggest_int('n_estimators', 100, 500),
            'max_depth': trial.suggest_int('max_depth', 3, 7),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),
            'subsample': trial.suggest_float('subsample', 0.6, 0.9),
            'random_state': 42, 'eval_metric': 'logloss'
        }
        model = XGBClassifier(**param)
        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        return cross_val_score(model, X_train_final, y_train, cv=cv, scoring='roc_auc').mean()

    study = optuna.create_study(direction='maximize')
    study.optimize(objective, n_trials=100)    
    # æŒä¹…åŒ– Study å¯¹è±¡
    joblib.dump(study, os.path.join(SAVE_DIR, f"optuna_xgboost_study_{target}.pkl"))
    print(f"âœ… Optuna å¯»ä¼˜å®Œæˆã€‚æœ€ä½³ AUC: {study.best_value:.4f}")
    
    # ä½¿ç”¨æœ€ä½³å‚æ•°é‡æ–°è®­ç»ƒ
    best_params = study.best_params
    best_xgb = XGBClassifier(**study.best_params, random_state=42, use_label_encoder=False, eval_metric='logloss')

    # =========================================================
    # 7. ğŸ† 5 ç§æ¨¡å‹ç®—æ³•ç«èµ› (å«æ¦‚ç‡æ ¡å‡†)
    # =========================================================
    models = {
        "Logistic Regression": LogisticRegression(max_iter=1000, class_weight='balanced'),
        "Decision Tree": DecisionTreeClassifier(max_depth=4, min_samples_leaf=20, class_weight='balanced'),
        "SVM": SVC(probability=True, kernel='rbf', C=1.0, class_weight='balanced'), 
        "Random Forest": RandomForestClassifier(n_estimators=200, max_depth=5, random_state=42, class_weight='balanced'),
        "XGBoost": best_xgb 
    }

    # å‡†å¤‡äºšç»„æµ‹è¯•ç´¢å¼•
    sub_mask = (sub_test == 1).values
    X_test_sub = X_test_final[sub_mask]
    y_test_sub = y_test.iloc[sub_mask]

    print("\n" + "="*70)
    print(f"{'Algorithm':<20} | {'Main AUC':<10} | {'No-Renal AUC':<10} | {'Brier':<10}")
    print("-" * 70)

    calibrated_results = {}
    for name, model in models.items():
        # ä½¿ç”¨æ¦‚ç‡æ ¡å‡†ä¼˜åŒ– Brier Score
        clf = CalibratedClassifierCV(model, cv=3, method='isotonic')
        clf.fit(X_train_final, y_train)
        
        y_prob = clf.predict_proba(X_test_final)[:, 1]
        auc_main = roc_auc_score(y_test, y_prob)
        brier = brier_score_loss(y_test, y_prob)
        
        y_prob_sub = clf.predict_proba(X_test_sub)[:, 1]
        auc_sub = roc_auc_score(y_test_sub, y_prob_sub)
        
        calibrated_results[name] = clf
        print(f"{name:<20} | {auc_main:.4f}      | {auc_sub:.4f}          | {brier:.4f}")

    # =========================================================
    # 7.1 ç»Ÿè®¡å­¦å¢å¼ºï¼šBootstrap è®¡ç®— 95% CI (å…¨äººç¾¤ + äºšç»„)
    # =========================================================
    from sklearn.utils import resample

    def get_auc_ci(model, X_test_data, y_test_data, n_bootstraps=1000):
        """é€šç”¨çš„ Bootstrap AUC ç½®ä¿¡åŒºé—´è®¡ç®—å‡½æ•°"""
        bootstrapped_scores = []
        for i in range(n_bootstraps):
            # ä½¿ç”¨ i ä½œä¸ºéšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯å¤ç°ä¸”æ¯æ¬¡é‡‡æ ·ä¸åŒ
            X_res, y_res = resample(X_test_data, y_test_data, random_state=i)
            if len(np.unique(y_res)) < 2: 
                continue
            prob = model.predict_proba(X_res)[:, 1]
            bootstrapped_scores.append(roc_auc_score(y_res, prob))
        
        sorted_scores = np.array(bootstrapped_scores)
        sorted_scores.sort()
        # è®¡ç®— 2.5% å’Œ 97.5% åˆ†ä½æ•°
        low = sorted_scores[int(0.025 * len(sorted_scores))]
        high = sorted_scores[int(0.975 * len(sorted_scores))]
        return low, high

    print("\n" + "="*110)
    print(f"{'Algorithm':<20} | {'Main AUC (95% CI)':<30} | {'No-Renal AUC (95% CI)':<30} | {'Brier':<8}")
    print("-" * 110)

    for name, clf in calibrated_results.items():
        # --- 1. å…¨äººç¾¤æŒ‡æ ‡ ---
        y_prob = clf.predict_proba(X_test_final)[:, 1]
        auc_main = roc_auc_score(y_test, y_prob)
        brier = brier_score_loss(y_test, y_prob)
        ci_low_m, ci_high_m = get_auc_ci(clf, X_test_final, y_test)
        main_auc_str = f"{auc_main:.3f} ({ci_low_m:.3f}-{ci_high_m:.3f})"
        
        # --- 2. äºšç»„ (No-Renal) æŒ‡æ ‡ ---
        # ä½¿ç”¨é¢„å…ˆå‡†å¤‡å¥½çš„ sub_mask æå–äºšç»„æ•°æ®
        ci_low_s, ci_high_s = get_auc_ci(clf, X_test_sub, y_test_sub)
        auc_sub = roc_auc_score(y_test_sub, clf.predict_proba(X_test_sub)[:, 1])
        sub_auc_str = f"{auc_sub:.3f} ({ci_low_s:.3f}-{ci_high_s:.3f})"
        
        # --- 3. æ‰“å°æ ¼å¼åŒ–ç»“æœ ---
        print(f"{name:<20} | {main_auc_str:<30} | {sub_auc_str:<30} | {brier:.4f}")
    print("="*110)

    # =========================================================
    # 7.2 æ€§èƒ½å¯¹æ¯”ç»˜å›¾ (å•å›¾å•æ–‡ä»¶ä¿å­˜)
    # =========================================================
    def save_final_plots(data_pairs, title_suffix, file_prefix):
        X_data, y_true = data_pairs
        
        # é¢„å…ˆè®¡ç®—æ‰€æœ‰æ¨¡å‹çš„æ¦‚ç‡ï¼Œç¡®ä¿ç»˜å›¾ä¸æ‰“å°ä¸€è‡´
        model_probs = {}
        for name, clf in calibrated_results.items():
            model_probs[name] = clf.predict_proba(X_data)[:, 1]

        # --- å›¾ A: çº¯ ROC æ›²çº¿ ---
        plt.figure(figsize=(9, 8))
        for name, y_prob in model_probs.items():
            fpr, tpr, _ = roc_curve(y_true, y_prob)
            auc_val = roc_auc_score(y_true, y_prob)
            plt.plot(fpr, tpr, label=f'{name} (AUC={auc_val:.3f})', lw=2)
        
        plt.plot([0, 1], [0, 1], color='gray', linestyle='--', lw=1)
        plt.title(f'ROC Curves\n({title_suffix})', fontsize=15, fontweight='bold')
        plt.xlabel('False Positive Rate', fontsize=12)
        plt.ylabel('True Positive Rate', fontsize=12)
        plt.legend(loc='lower right', fontsize=10)
        plt.grid(alpha=0.2)
        plt.savefig(os.path.join(FIG_DIR, f"Figure_ROC_{file_prefix}_{target}.png"), dpi=300, bbox_inches='tight')
        plt.show()
        plt.close()

        # --- å›¾ B: çº¯ Calibration æ›²çº¿ ---
        plt.figure(figsize=(9, 8))
        for name, y_prob in model_probs.items():
            prob_true, prob_pred = calibration_curve(y_true, y_prob, n_bins=10)
            plt.plot(prob_pred, prob_true, marker='o', label=name, markersize=6, lw=2)
            
        plt.plot([0, 1], [0, 1], color='gray', linestyle='--', label='Perfectly Calibrated')
        plt.title(f'Calibration Curves\n({title_suffix})', fontsize=15, fontweight='bold')
        plt.xlabel('Predicted Probability', fontsize=12)
        plt.ylabel('Actual Probability', fontsize=12)
        plt.legend(loc='upper left', fontsize=10)
        plt.grid(alpha=0.2)
        plt.savefig(os.path.join(FIG_DIR, f"Figure_Calib_{file_prefix}_{target}.png"), dpi=300, bbox_inches='tight')
        plt.show()
        plt.close()
        

    # --- æœ€ç»ˆæ‰§è¡Œï¼šç”Ÿæˆ 4 å¼ ç‹¬ç«‹å›¾ç‰‡ ---
    print("\nğŸ“Š æ­£åœ¨ç”Ÿæˆ 4 å¼ ç‹¬ç«‹çš„è®ºæ–‡æ’å›¾ (ROC & Calibration for Train/Val)...")
    # éªŒè¯é›†å›¾ (å¯¹åº”ä½ ç»ˆç«¯è¾“å‡ºçš„ 0.83 å·¦å³)
    save_final_plots((X_test_final, y_test), "Validation Group", "Validation")
    # è®­ç»ƒé›†å›¾ (å¯¹åº”ä½ çœ‹åˆ°çš„ 0.90 å·¦å³)
    save_final_plots((X_train_final, y_train), "Training Group", "Training")
    # =========================================================
    # 8. å…¨èµ„äº§ä¿å­˜ (ç¡®ä¿æ¯ä¸ª Outcome ç‹¬ç«‹ä¿å­˜)
    # =========================================================
    # ä¿å­˜æ¨¡å‹å­—å…¸
    joblib.dump(calibrated_results, os.path.join(SAVE_DIR, f"all_models_{target}.pkl"))
    # --- [æ–°å¢] è‡ªåŠ¨ä¿å­˜ç½®ä¿¡åŒºé—´ (CI) å®¡è®¡æ•°æ® ---
    ci_audit_data = {}
    sub_ci_audit_data = {}

    for name, clf in calibrated_results.items():
        # 1. è®¡ç®—å…¨äººç¾¤ CI
        ci_low_m, ci_high_m = get_auc_ci(clf, X_test_final, y_test)
        auc_main = roc_auc_score(y_test, clf.predict_proba(X_test_final)[:, 1])
        ci_audit_data[name] = f"{auc_main:.3f} ({ci_low_m:.3f}-{ci_high_m:.3f})"
        
        # 2. è®¡ç®—äºšç»„ CI
        ci_low_s, ci_high_s = get_auc_ci(clf, X_test_sub, y_test_sub)
        auc_sub = roc_auc_score(y_test_sub, clf.predict_proba(X_test_sub)[:, 1])
        sub_ci_audit_data[name] = f"{auc_sub:.3f} ({ci_low_s:.3f}-{ci_high_s:.3f})"

    # ä¿å­˜ CI å­—å…¸ï¼Œä¾›æ¨¡å— 04 ç›´æ¥è°ƒç”¨
    joblib.dump(ci_audit_data, os.path.join(SAVE_DIR, f"ci_main_{target}.pkl"))
    joblib.dump(sub_ci_audit_data, os.path.join(SAVE_DIR, f"ci_sub_{target}.pkl"))
    print(f"ğŸ“Š {target} çš„ç½®ä¿¡åŒºé—´æ•°æ®å·²è‡ªåŠ¨åŒæ­¥è‡³æœ¬åœ°æ–‡ä»¶ã€‚")
    # ä¿å­˜è¯¥ç»“å±€ç­›é€‰å‡ºçš„ Top 12 ç‰¹å¾å
    joblib.dump(selected_features, os.path.join(SAVE_DIR, f"selected_features_{target}.pkl"))
    
    # ä¿å­˜æµ‹è¯•é›†æ•°æ®ï¼Œæ–¹ä¾¿åç»­ç¦»çº¿åš SHAP æˆ–å…¶ä»–åˆ†æ
    X_test_final_df = pd.DataFrame(X_test_final, columns=selected_features)
    joblib.dump((X_test_final_df, y_test), os.path.join(SAVE_DIR, f"test_data_main_{target}.pkl"))
    joblib.dump((X_test_sub, y_test_sub), os.path.join(SAVE_DIR, f"test_data_sub_{target}.pkl"))

    # =========================================================
    # 9. æ„å»ºæœ€ç»ˆæ€§èƒ½æ±‡æ€»æŠ¥è¡¨
    # =========================================================
    current_outcome_summary = [] # ä½¿ç”¨æ›´æ˜ç¡®çš„å˜é‡å
    
    for name, clf in calibrated_results.items():
        # æ‰§è¡Œ Bootstrap è®¡ç®—å…¨äººç¾¤å’Œäºšç»„çš„ 95% CI
        ci_low_m, ci_high_m = get_auc_ci(clf, X_test_final, y_test)
        ci_low_s, ci_high_s = get_auc_ci(clf, X_test_sub, y_test_sub)
        
        # è®¡ç®—å…¨äººç¾¤æŒ‡æ ‡
        y_prob = clf.predict_proba(X_test_final)[:, 1]
        auc_main = roc_auc_score(y_test, y_prob)
        brier = brier_score_loss(y_test, y_prob)
        
        # è®¡ç®—äºšç»„ (No-Renal) æŒ‡æ ‡
        y_prob_sub = clf.predict_proba(X_test_sub)[:, 1]
        auc_sub = roc_auc_score(y_test_sub, y_prob_sub)

        # æ•´ç†æˆå­—å…¸ï¼Œæ·»åŠ è¿›åˆ—è¡¨
        current_outcome_summary.append({
            "Outcome": target,
            "Algorithm": name,
            "Main AUC": round(auc_main, 4),
            "Main AUC (95% CI)": f"{auc_main:.3f} ({ci_low_m:.3f}-{ci_high_m:.3f})",
            "No-Renal AUC": round(auc_sub, 4),
            "No-Renal AUC (95% CI)": f"{auc_sub:.3f} ({ci_low_s:.3f}-{ci_high_s:.3f})",
            "Brier Score": round(brier, 4)
        })

    print("-" * 60)
    print(f"âœ… ç»“å±€ {target.upper()} åˆ†æåŠèµ„äº§ä¿å­˜æˆåŠŸï¼")
    

    train_assets = {
        'medians': X_train.median().to_dict(), # è¯¥ç»“å±€å¯¹åº”çš„è®­ç»ƒé›†ä¸­ä½æ•°
        'skewed_cols': existing_skewed,        # åæ€å¤„ç†åˆ—è¡¨
        'selected_features': selected_features # è¯¥ç»“å±€ç­›é€‰å‡ºçš„ Top 12
    }
    
    # æ–‡ä»¶åå¸¦ä¸Š target åç¼€ï¼Œå¦‚ train_assets_pof.pkl
    assets_save_path = os.path.join(SAVE_DIR, f"train_assets_{target}.pkl")
    joblib.dump(train_assets, assets_save_path)

    # åŒæ­¥ä¿å­˜ä¸€ä»½ä¸“å±ç‰¹å¾æ¸…å•ï¼Œæ–¹ä¾¿å…¶ä»–æ¨¡å—è°ƒç”¨
    joblib.dump(selected_features, os.path.join(SAVE_DIR, f"selected_features_{target}.pkl"))

    print(f"ğŸ“¦ [èµ„äº§åŒæ­¥] ä¸“å±åŸºå‡†å·²å­˜è‡³: {assets_save_path}")
    print(f"ğŸ“¦ [ç‰¹å¾åŒæ­¥] ä¸“å±ç‰¹å¾æ¸…å•å·²å­˜è‡³: selected_features_{target}.pkl")
        
    return current_outcome_summary

if __name__ == "__main__":
    run_module_03_all_outcomes()
