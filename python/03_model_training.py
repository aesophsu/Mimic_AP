import os
import pandas as pd
import numpy as np
import joblib
import optuna

# æœºå™¨å­¦ä¹ æ ¸å¿ƒåº“
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.linear_model import LogisticRegression, LassoCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.calibration import CalibratedClassifierCV
from sklearn.metrics import roc_auc_score, brier_score_loss

# å±è”½è­¦å‘Š
import warnings
warnings.filterwarnings('ignore')
optuna.logging.set_verbosity(optuna.logging.WARNING)

# =========================================================
# 1. é…ç½®ä¸è·¯å¾„
# =========================================================
BASE_DIR = ".."
INPUT_PATH = os.path.join(BASE_DIR, "data/cleaned/mimic_for_model.csv")
SAVE_DIR = os.path.join(BASE_DIR, "models")
if not os.path.exists(SAVE_DIR):
    os.makedirs(SAVE_DIR)

def run_module_03_optimized():
    print("="*60)
    print("ğŸš€ è¿è¡Œç»ˆæé‡æ„æ¨¡å— 03: 5 ç§æ¨¡å‹ç«èµ› + åŠ¨æ€å¯¹æ•°å¤„ç†")
    print("="*60)
    
    if not os.path.exists(INPUT_PATH):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶ {INPUT_PATH}")
        return
        
    df = pd.read_csv(INPUT_PATH)

    # =========================================================
    # 2. ç‰¹å¾æ¸…æ´—ä¸é¢„å¤„ç† (å…³é”®ï¼šä¿®å¤æ–‡æœ¬åˆ—æŠ¥é”™)
    # =========================================================
    if 'gender' in df.columns:
        df['gender'] = df['gender'].map({'M': 1, 'F': 0})
    
    target = 'pof'
    # æ’é™¤åˆ—è¡¨
    drop_list = [
        target, 'mortality_28d', 'renal_pof', 'resp_pof', 'cv_pof', 
        'subgroup_no_renal', 'hosp_mortality', 'overall_mortality',
        'composite_outcome'
    ]
    
    # ğŸ›¡ï¸ è‡ªåŠ¨å‰”é™¤éæ•°å€¼ç‰¹å¾ (å¤„ç† ValueError: could not convert string to float)
    text_cols = df.select_dtypes(include=['object']).columns.tolist()
    final_drop = list(set(drop_list + text_cols))
    print(f"ğŸ—‘ï¸ è‡ªåŠ¨å‰”é™¤æ³„éœ²/éæ•°å€¼ç‰¹å¾: {text_cols}")
    
    X = df.drop(columns=[c for c in final_drop if c in df.columns])
    y = df[target]
    subgroup_flag = df['subgroup_no_renal']

    # å¤„ç†æ— ç©·å¤§å¹¶ç¡®ä¿æ•°å€¼åŒ–
    X = X.replace([np.inf, -np.inf], np.nan).astype(float)
    
    # åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†
    X_train, X_test, y_train, y_test, sub_train, sub_test = train_test_split(
        X, y, subgroup_flag, test_size=0.2, random_state=42, stratify=y
    )

    # =========================================================
    # 3. ğŸ§ª æ ¸å¿ƒä¿®æ­£ï¼šåŠ¨æ€ Log1p è½¬æ¢ (æ•‘èµçº¿æ€§æ¨¡å‹)
    # =========================================================
    skewed_cols = ['creatinine_max', 'creatinine_min', 'bun_max', 'bun_min',
                   'wbc_max', 'wbc_min', 'glucose_max', 'glucose_min',
                   'lab_amylase_max', 'lipase_max', 'lactate_max',
                   'alt_max', 'ast_max', 'bilirubin_total_max', 
                   'alp_max', 'inr_max', 'rdw_max']
    
    existing_skewed = [c for c in skewed_cols if c in X_train.columns]
    print(f"ğŸ”„ æ­£åœ¨æ‰§è¡ŒåŠ¨æ€ Log1p è½¬æ¢ ({len(existing_skewed)} ä¸ªå˜é‡)...")
    for col in existing_skewed:
        X_train[col] = np.log1p(X_train[col].clip(lower=0))
        X_test[col] = np.log1p(X_test[col].clip(lower=0))

    # =========================================================
    # 4. å¢å¼ºå‹å¤šé‡æ’è¡¥ (MICE) & æ ‡å‡†åŒ–
    # =========================================================
    print("ğŸ§ª æ­£åœ¨æ‰§è¡Œæ·±åº¦æ’è¡¥ (MICE)...")
    mice_imputer = IterativeImputer(max_iter=20, random_state=42, initial_strategy='median')
    scaler = StandardScaler()

    X_train_imp = mice_imputer.fit_transform(X_train)
    X_train_std = scaler.fit_transform(X_train_imp)

    X_test_imp = mice_imputer.transform(X_test)
    X_test_std = scaler.transform(X_test_imp)

    # ä¿å­˜èµ„äº§ä¾›è·¨åº“éªŒè¯
    joblib.dump(scaler, os.path.join(SAVE_DIR, "scaler.pkl"))
    joblib.dump(mice_imputer, os.path.join(SAVE_DIR, "mice_imputer.pkl"))
    joblib.dump(existing_skewed, os.path.join(SAVE_DIR, "skewed_cols.pkl"))

    # =========================================================
    # 5. LASSO ç‰¹å¾é™ç»´ (Top 12)
    # =========================================================
    print("ğŸ§ª æ­£åœ¨ç²¾é€‰æè‡´æ ¸å¿ƒç‰¹å¾ (Top 12)...")
    lasso = LassoCV(cv=5, random_state=42, max_iter=20000).fit(X_train_std, y_train)
    
    coef_abs = np.abs(lasso.coef_)
    indices = np.argsort(coef_abs)[-12:] # é”å®šç»å¯¹å€¼æœ€å¤§çš„ 12 ä¸ªç‰¹å¾
    selected_features = X.columns[indices].tolist()
    
    X_train_final = X_train_std[:, indices]
    X_test_final = X_test_std[:, indices]
    print(f"âœ… ç‰¹å¾ç²¾ç®€å®Œæˆ: {selected_features}")

    # =========================================================
    # 6. XGBoost Optuna è¶…å‚æ•°å¯»ä¼˜
    # =========================================================
    print("\nğŸ”¬ å¯åŠ¨ XGBoost è´å¶æ–¯å¯»ä¼˜ (Optuna)...")
    def objective(trial):
        param = {
            'n_estimators': trial.suggest_int('n_estimators', 100, 500),
            'max_depth': trial.suggest_int('max_depth', 3, 7),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),
            'subsample': trial.suggest_float('subsample', 0.6, 0.9),
            'random_state': 42, 'eval_metric': 'logloss'
        }
        model = XGBClassifier(**param)
        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        return cross_val_score(model, X_train_final, y_train, cv=cv, scoring='roc_auc').mean()

    study = optuna.create_study(direction='maximize')
    study.optimize(objective, n_trials=100)
    best_xgb = XGBClassifier(**study.best_params)

    # =========================================================
    # 7. ğŸ† 5 ç§æ¨¡å‹ç®—æ³•ç«èµ› (å«æ¦‚ç‡æ ¡å‡†)
    # =========================================================
    models = {
        "Logistic Regression": LogisticRegression(max_iter=1000),
        "Decision Tree": DecisionTreeClassifier(max_depth=4, min_samples_leaf=20),
        "SVM": SVC(probability=True, kernel='rbf', C=1.0), 
        "Random Forest": RandomForestClassifier(n_estimators=200, max_depth=5, random_state=42),
        "XGBoost": best_xgb
    }

    # å‡†å¤‡äºšç»„æµ‹è¯•ç´¢å¼•
    sub_mask = (sub_test == 1).values
    X_test_sub = X_test_final[sub_mask]
    y_test_sub = y_test.iloc[sub_mask]

    print("\n" + "="*70)
    print(f"{'Algorithm':<20} | {'Main AUC':<10} | {'No-Renal AUC':<10} | {'Brier':<10}")
    print("-" * 70)

    calibrated_results = {}
    for name, model in models.items():
        # ä½¿ç”¨æ¦‚ç‡æ ¡å‡†ä¼˜åŒ– Brier Score
        clf = CalibratedClassifierCV(model, cv=3, method='isotonic')
        clf.fit(X_train_final, y_train)
        
        y_prob = clf.predict_proba(X_test_final)[:, 1]
        auc_main = roc_auc_score(y_test, y_prob)
        brier = brier_score_loss(y_test, y_prob)
        
        y_prob_sub = clf.predict_proba(X_test_sub)[:, 1]
        auc_sub = roc_auc_score(y_test_sub, y_prob_sub)
        
        calibrated_results[name] = clf
        print(f"{name:<20} | {auc_main:.4f}     | {auc_sub:.4f}         | {brier:.4f}")

    # =========================================================
    # 8. å…¨èµ„äº§ä¿å­˜
    # =========================================================
    joblib.dump(calibrated_results, os.path.join(SAVE_DIR, "all_models.pkl"))
    joblib.dump(selected_features, os.path.join(SAVE_DIR, "selected_features.pkl"))
    
    # ä¿å­˜æµ‹è¯•é›† DataFrame æ ¼å¼ä¾›æ¨¡å— 08 ä½¿ç”¨
    X_test_final_df = pd.DataFrame(X_test_final, columns=selected_features)
    joblib.dump((X_test_final_df, y_test), os.path.join(SAVE_DIR, "test_data_main.pkl"))
    
    print("-" * 60)
    print("âœ… æ¨¡å— 03 æˆåŠŸï¼çº¿æ€§æ¨¡å‹ä¸æ ‘æ¨¡å‹å·²å®ŒæˆåŠ¨æ€å¤„ç†å¹¶ä¿å­˜ã€‚")

if __name__ == "__main__":
    run_module_03_optimized()
