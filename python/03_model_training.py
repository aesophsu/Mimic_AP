import os
import pandas as pd
import numpy as np
import joblib

# æœºå™¨å­¦ä¹ æ ¸å¿ƒåº“
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.linear_model import LogisticRegression, LassoCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.calibration import CalibratedClassifierCV
from sklearn.metrics import roc_auc_score, brier_score_loss

# =========================================================
# 1. é…ç½®ä¸è·¯å¾„
# =========================================================
BASE_DIR = ".."
INPUT_PATH = os.path.join(BASE_DIR, "data/cleaned/mimic_for_model.csv")
SAVE_DIR = os.path.join(BASE_DIR, "models")
if not os.path.exists(SAVE_DIR):
    os.makedirs(SAVE_DIR)

def run_module_03():
    print("="*60)
    print("ğŸš€ è¿è¡Œä¼˜åŒ–æ¨¡å— 03: æè‡´ç²¾ç®€ç‰¹å¾ + é²æ£’æ’è¡¥ + ç®—æ³•ç«èµ›")
    print("="*60)
    
    if not os.path.exists(INPUT_PATH):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶ {INPUT_PATH}")
        return
        
    df = pd.read_csv(INPUT_PATH)

    # =========================================================
    # 2. ç‰¹å¾æ¸…æ´—ä¸é¢„å¤„ç†
    # =========================================================
    # ç¼–ç æ€§åˆ«
    if 'gender' in df.columns:
        df['gender'] = df['gender'].map({'M': 1, 'F': 0})
    
    # å®šä¹‰æ’é™¤åˆ—è¡¨ (IDã€ç»“å±€ã€è¯„åˆ†ã€ä»¥åŠå¯¼è‡´æ³„éœ²çš„ç‰¹å¾)
    target = 'pof'
    drop_list = [
        target, 'mortality_28d', 'renal_pof', 'resp_pof', 'cv_pof', 
        'subgroup_no_renal', 'hosp_mortality', 'overall_mortality',
        'subject_id', 'hadm_id', 'stay_id', 'admittime', 'dischtime', 'intime', 
        'race', 'insurance'
    ]
    
    X = df.drop(columns=[c for c in drop_list if c in df.columns])
    y = df[target]
    
    # è®°å½•äºšç»„æ ‡è®°ï¼Œç”¨äºåç»­éªŒè¯
    subgroup_flag = df['subgroup_no_renal']

    # å¼ºåˆ¶æ•°å€¼åŒ–å¹¶å¤„ç†å¼‚å¸¸å€¼
    X = X.replace([np.inf, -np.inf], np.nan).astype(float)
    
    # åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›† (80/20)
    X_train, X_test, y_train, y_test, sub_train, sub_test = train_test_split(
        X, y, subgroup_flag, test_size=0.2, random_state=42, stratify=y
    )

    # =========================================================
    # 3. å¢å¼ºå‹å¤šé‡æ’è¡¥ (è¿›ä¸€æ­¥ä¼˜åŒ–æ”¶æ•›æ€§)
    # =========================================================
    print("ğŸ§ª æ­£åœ¨æ‰§è¡Œæ·±åº¦æ’è¡¥ (MICE)...")
    # å¢åŠ è¿­ä»£æ¬¡æ•°è‡³ 40ï¼Œé™åˆ¶å‚ä¸é¢„æµ‹çš„ç‰¹å¾æ•°ä¸º 10 ä»¥å‡å°‘å…±çº¿æ€§å¹²æ‰°
    mice_imputer = IterativeImputer(
        max_iter=40, 
        n_nearest_features=10, 
        tol=1e-3, 
        random_state=42,
        initial_strategy='median'
    )
    scaler = StandardScaler()

    X_train_imp = mice_imputer.fit_transform(X_train)
    X_test_imp = mice_imputer.transform(X_test)
    
    X_train_std = scaler.fit_transform(X_train_imp)
    X_test_std = scaler.transform(X_test_imp)

    # =========================================================
    # 4. LASSO ç‰¹å¾é™ç»´ + æè‡´ç²¾é€‰ (Top 12)
    # =========================================================
    # ç†ç”±ï¼š12ä¸ªç‰¹å¾åœ¨ä¸´åºŠè®ºæ–‡ä¸­æ›´æ˜“å±•ç¤ºï¼Œä¸”é€šå¸¸å·²èƒ½æ•æ‰ 95% ä»¥ä¸Šçš„é¢„æµ‹æ•ˆèƒ½
    print("ğŸ§ª æ­£åœ¨ç²¾é€‰æè‡´æ ¸å¿ƒç‰¹å¾ (Top 12)...")
    lasso = LassoCV(cv=5, random_state=42, max_iter=20000).fit(X_train_std, y_train)
    
    # è·å–ç³»æ•°ç»å¯¹å€¼æ’åº
    coef_series = pd.Series(np.abs(lasso.coef_), index=X.columns).sort_values(ascending=False)
    # é€‰å–å‰ 12 ä¸ªæœ€é‡è¦çš„ç‰¹å¾ï¼ˆæˆ–æ‰€æœ‰éé›¶ç‰¹å¾ï¼Œå–ä¸¤è€…ä¸­å°çš„é‚£ä¸€ä¸ªï¼‰
    num_features = min(12, (lasso.coef_ != 0).sum())
    selected_features = coef_series.head(num_features).index.tolist()
    
    # é‡æ–°è·å–é€‰å®šç‰¹å¾çš„ç´¢å¼•
    feature_idx = [X.columns.get_loc(c) for c in selected_features]
    X_train_final = X_train_std[:, feature_idx]
    X_test_final = X_test_std[:, feature_idx]
    
    print(f"âœ… ç‰¹å¾æè‡´ç²¾ç®€å®Œæˆï¼šä¿ç•™äº† {len(selected_features)} ä¸ªä¸´åºŠæœ€æ ¸å¿ƒæŒ‡æ ‡ã€‚")
    print(f"ğŸ“ æœ€ç»ˆæŒ‡æ ‡æ¸…å•: {selected_features}")

    # =========================================================
    # 5. å¤šç®—æ³•ç«èµ›ä¸æ¦‚ç‡æ ¡å‡†
    # =========================================================
    models = {
        "Logistic Regression": LogisticRegression(max_iter=1000),
        "Decision Tree": DecisionTreeClassifier(max_depth=4, min_samples_leaf=30),
        "SVM": SVC(probability=True, kernel='rbf', C=1.0), 
        "Random Forest": RandomForestClassifier(n_estimators=150, max_depth=6, random_state=42),
        "XGBoost": XGBClassifier(n_estimators=100, learning_rate=0.05, max_depth=4, eval_metric='logloss', random_state=42)
    }

    print("\nğŸ† æ¨¡å‹æ€§èƒ½å¯¹æ¯” (AUC & Brier Score):")
    calibrated_results = {}
    
    for name, model in models.items():
        # ä¸´åºŠç ”ç©¶æ ¸å¿ƒï¼šæ¦‚ç‡æ ¡å‡†
        clf = CalibratedClassifierCV(model, cv=3, method='isotonic')
        clf.fit(X_train_final, y_train)
        
        y_prob = clf.predict_proba(X_test_final)[:, 1]
        auc_val = roc_auc_score(y_test, y_prob)
        brier_val = brier_score_loss(y_test, y_prob)
        
        calibrated_results[name] = clf
        print(f"   - {name:<20}: AUC = {auc_val:.4f} | Brier = {brier_val:.4f}")

    # =========================================================
    # 6. ä¿å­˜æ ¸å¿ƒç»“æœç”¨äºå¯è§†åŒ–
    # =========================================================
    # å­˜å‚¨è·¯å¾„
    joblib.dump(calibrated_results, os.path.join(SAVE_DIR, "all_models.pkl"))
    joblib.dump(selected_features, os.path.join(SAVE_DIR, "selected_features.pkl"))
    
    # ä¿å­˜å…¨é›†æµ‹è¯•æ•°æ® (DataFrameæ ¼å¼æ–¹ä¾¿åç»­SHAPå¤„ç†)
    X_test_final_df = pd.DataFrame(X_test_final, columns=selected_features)
    joblib.dump((X_test_final_df, y_test), os.path.join(SAVE_DIR, "test_data_main.pkl"))
    
    # ä¿å­˜ No-Renal äºšç»„æµ‹è¯•æ•°æ®
    subgroup_mask = (sub_test == 1).values
    X_test_sub = X_test_final_df[subgroup_mask]
    y_test_sub = y_test.iloc[subgroup_mask] if isinstance(y_test, pd.Series) else y_test[subgroup_mask]
    joblib.dump((X_test_sub, y_test_sub), os.path.join(SAVE_DIR, "test_data_subgroup.pkl"))

    print("-" * 60)
    print(f"âœ… æ¨¡å— 03 è¿è¡ŒæˆåŠŸï¼äºšç»„æµ‹è¯•æ ·æœ¬: {len(y_test_sub)}ï¼Œå…¨é›†æµ‹è¯•æ ·æœ¬: {len(y_test)}")
    print("="*60)

if __name__ == "__main__":
    run_module_03()
